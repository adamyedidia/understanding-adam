<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style type="text/css">@import "vis/playground.css";</style>
  
  <script src="vis/js/d3.min.js" charset="utf-8"></script>
  <script src="vis/js/underscore.js" charset="utf-8"></script>
  <script src="vis/js/rounding.js" charset="utf-8"></script>
  <script src="vis/util.js" charset="utf-8"></script>
  <script src="vis/GridWorld.js" charset="utf-8"></script>
  <script src="vis/LearnQV.js" charset="utf-8"></script>
  <script src="vis/Policy.js" charset="utf-8"></script>
  <script src="vis/Aprox.js" charset="utf-8"></script>

  <script src="vis/cliff.js" charset="utf-8"></script>
  <script src="vis/compare.js" charset="utf-8"></script>
  <script src="vis/tug.js" charset="utf-8"></script>
  <script src="vis/tug_baseline.js" charset="utf-8"></script>
  <script src="vis/reinforce.js" charset="utf-8"></script>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "TD Learning as Intersecting Paths",
  "description": "",
  "password": "td",
  "authors": [

    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }

    {
      "author": "Sam Greydanus",
      "authorURL": "https://greydanus.github.io/about.html",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "strict": false,
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<style>
  #cliff_iter .cell rect {
    fill: #e7eae7;
  }
  .cell rect {
    fill: #e7eae7;
  }
</style>

<d-title>
  <h1>TD Learning as Intersecting Paths</h1>
  <p>Temporal Difference (TD) learning merges paths for greater statistical efficiency. What are the consequences?</p>
  <figure class="l-page">
    <div id="compare_hero" style="width:1000px; height:350px; margin:auto; position:relative;"> </div>
  </figure>
  <script> compare_vis(d3.select("#compare_hero"), {algs: ["MC", "TD", "Q"] }); </script> 
</d-title>

<d-article>



    <!-- Estimating value -->
<!--     <p>
      After making an important decision, we often ask ourselves, "What might have happened if we had acted differently?" One of the most beautiful and infuriating parts of life is that there is no answer to this question. The best we can do is use our past experiences to guess what <i>might</i> have happened. Sometimes, though, these guesses can be quite good. They allow us to make good choices, even when we are placed in situations we have been in before.
    </p> -->

<!--     <p>
      The dilemma of how to choose among several actions without being <i>certain</i> about their consequences is central to reinforcement learning. In this article, we consider several ways an agent can learn from experience. We interpret experience as a series of paths through an environmnet and then show how Temporal Difference (TD) methods, unlike Monte Carlo methods, merges these paths where they intersect to increase statistical efficiency.
    </p> -->

    <p>
      After making an important decision, we often ask ourselves, "What might have happened if I had acted differently?" One of the most beautiful but daunting parts of life is that there is no way to answer to this question. The best we can do is use our past experiences to guess what <i>might</i> have happened...
    </p>

    <p>
      Reinforcement learning (RL) agents face the same issue: they cannot choose more than one action at a time. They, too, can never know the relative outcomes of two actions. In order to learn from experience, RL agents maintain a concept of state. States summarize the agent-environment relationship and tend to recur over time. States allow agents to draw analogies between past and future decisions, thus making better decisions over time.

    <p>
      Monte Carlo (MC) Learning and Temporal Difference (TD) Learning are two methods that RL agents use to do this. Sometimes we think of TD learning as a practical approximation of MC. But there is more to the story: TD methods sometimes outperform MC methods by a large margin, something that we wouldn't expect from an approximation. In this post, we introduce an intuition based on the idea that an agent's experience is a set of paths. We show that TD methods merge these paths for greater statistical efficiency whereas MC methods keep them separate. Merging paths lets TD outperform Monte Carlo so long as the intersections are well-chosen. Otherwise, TD ends up being worse than Monte Carlo.
    </p>

    <h3>The Reinforcement Learning Paradigm</h3>

    <p>
      The reinforcement learning paradigm begins with an agent and an environment. The agent's goal is to maximize its rewards by interacting with the environmnet. The poor environment responds with rewards and asks nothing in return.
    </p>
    <p>
      Interactions between agent and environment happen over one or more discrete intervals of time, or "time steps." At each time step $$t$$, the agent makes an observation $$o_t$$ of the environment and combines it with previous experience to compute a state $$s_t$$. Then, the agent uses past experiences with similar states to select an action $$a_t$$. The environment responds with a reward $$r_{t+1}$$ and a new observation $$o_{t+1}$$, and the cycle repeats.
    </p>

    <p>
      For state $$s$$, we consider an agent that selects action $$a$$ with probability $$\pi(s,a)$$. We call $$\pi$$ the agent's <b>policy function</b>. When the agent uses its policy to interact with the environment, it gains <b>experience</b> $$E$$. You can think of $$E$$ as the som of the agent's interactions with the environment, where each interaction is a $$(s,a,r)$$ tuple. When the agent interacts with the environment over the course of several time steps, we can think of the experience as being made up of a "path" of these tuples.
    </p>

    <figure id="figure-concepts" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/concepts.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">1</a>:
        Four key concepts for learning from experience.
      </figcaption>
    </figure>

    <p>
      Often, the hardest part of reinforcement learning is converting an agent's experience into a better policy. In this post, we'll assume that we don't have access to an underlying model of the environment's dynamics
      <d-footnote>
          This is the model-free assumption in RL. There is a vibrant area of research around model-based RL where this is not the case.
      </d-footnote>.
      Instead, we will focus on using experience to learn a <b>state-value function</b> $$V(s)$$ or an <b>action-value function</b> $$Q(s,a)$$. With these functions, we can choose better policies.
    </p>

    <p>
      Value functions estimate the sum of future rewards that an agent can expect to receive, given its context (e.g state for $$V(s)$$) and policy. As we explore two methods of using experience to estimate value, we'll find that there are some nuances to this idea.
    </p>

<!--     </p>
      <li> <b>Policy.</b> An agent chooses how to interact with its environment using a policy function $$\pi$$. For any state $$s$$, the agent chooses action $$a$$ with probability $$\pi(s,a)$$.</li>

      <li> <b>Experience.</b> Experience is the sum of an agent's interactions with its environment. Each interaction is includes the agent's state, the action it chose, and the reward $$r$$ it received. We can write this as a tuple, $$(s,a,r)$$. As an agent explores its environment, it builds up a "path" of these tuples, one for each time step. Think of each episode in the agent's experience as a different path.</li>

      <li> <b>Value functions.</b> An agent uses value functions to make generalizations about its experience. We'll consider two different value functions: the state-value function $$V(s)$$ and the action-value function $$Q(s,a)$$. Each of these functions measures the agent's expected future rewards, or value, when it is in a given context. As you can see, the action-value function is a bit more specific because it considers the case when an agent is in a specific state <i>and</i> has chosen an action.</li>

      <li> <b>State-value function.</b> An agent uses value functions to make generalizations about its experience. To be specific, value functions measure an agent's expected future rewards given a specific context, which is called the <i>value</i> of the context. The state-value function $$V(s)$$, for example, measures an agent's expected rewards in the contect of a state.</li>

      <li> <b>Action-value function.</b> The action-value function $$Q(s,a)$$, meanwhile, measures an agent's expected future rewards in the contect of a particular state-action pair. As you can see, Q values are a bit more specific than state values.</li>
    </p> -->


    <h2>Policy Value Iteration</h2>
    <br>

    <p>
      You may have noticed that there is a natural cycle from an agent's policy to experience to value estimate, and back again. As the agent follows this cycle, its policy improves, its experience increases, and its value estimates become more precise. The agent "grows up." This is a process called <i>policy value iteration</i>
        <d-footnote>
          Sometimes people lump experience into the policy step, hence "policy value iteration" rather than "policy experience value iteration." But we chose not to do this because the "paths" interpretation of experience is important to our thesis.
        </d-footnote>
    </p>

    <figure id="figure-policy-value-iteration" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/policy-value-iteration.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">2</a>:
        Policy value iteration.
      </figcaption>
    </figure>

    <p>
      Every reinforcement learning algorithm performs policy iteration in one form or another. The key differences among algorithms lie in the way each of the three properties get <i>updated</i>. As you can imagine, a small difference in the way an agent changes its policy might have big consequences for the way it collects experience, value estimates, and in turn updates its policy the next time.
    </p>

    <p>
      <b>Running the policy.</b> This is how the agent updates its experience. Some algorithms do this for only one time step, whereas others will run an entire episode or batch of episodes. Some algorithms keep a replay memory of all the agent's training experience whereas others do not store experience at all.
    </p>


    <p>
      <b>Value learning.</b> In this step, the agent updates its value estimate.
    </p>

      <center><p>$$R = \sum_t \gamma^t r_t$$ </p> </center>

    <p>
      The most obvious way to estimate value from returns is just to take the average:.
      <center><p>$$\hat V(s_i) = \frac{1}{N} \sum_{s'=s} R_{s'}$$ </p> </center>
      This is just Monte Carlo value estimation. The surprising fact is that we can do better than this.

    </p>

    <p>
      <b>Policy update.</b> Once the agent has refined its value estimate, ...
    </p>

    <p>
      <b>THE WRITING AFTER THIS IS DEPRECATED.</b> I need to make it sound better.
    </p>

    <p>
      <b>Value learning.</b> Learning value is often the hardest part of policy iteration. Let's consider state-value estimation here, but we will extend the discussion to action-value estimation later. The first step is to define value. Value is the sum of future rewards an agent expects to receive, given a particular context (state or state-action pair). In practice, this means computing a running average over all the outcomes of paths that pass through 


      As we sketch an outline of value estimation here, keep in mind that the remainder of the article will be devoted to its nuances.
      </p>

    <p>
      The first step of value estimation is <i>credit assignment</i>: given a series of states leading up to a reward, who gets the credit? One of the few assumptions we can make is that states separated by fewer time steps probably contributed more. In this case, the amount of reward (or credit) assigned to each state is equal to the sum of time-discounted rewards:
    </p>

    <center><p>$$R = \sum_t \gamma^t r_t$$ </p> </center>

    <p>
      In the equation above, $$\gamma$$ is a constant between zero and one which determines how much a reward should be discounted across time steps. Now, for tuple in our experience, we have an estimate of the future reward. The simplest way of computing the expected future reward at state $$s_i$$, now, is just to take the average return for all tuples containing $$s_i$$
    </p>

    <center><p>$$\hat V(s_i) = \frac{1}{N} \sum_{s'=s} R_{s'}$$ </p> </center>

    <p>
      Having defined return, the simplest way to estimate a state's value would be to take the average of all returns associated with that state. That is how Monte Carlo learning works! Surprisingly, we'll see later that you can do better than this using TD methods.
    </p>

    


    <h3>Example: Cliff walking</h3>
    <br>

    <figure id="figure-cliff-walking" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/cliff-walking.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">3</a>:
        Cliff walking.
      </figcaption>
    </figure>
<!--     <figure class="l-page">
    <div id="cliff_iter" style="width:600px; height:500px; position:relative;"> </div>
    </figure>
    <script> cliff_vis(d3.select("#cliff_iter")); </script> -->




    <h2>Aside: Function approximation</h2>
    <ul>
      <li>Evaluate: $$~~~f(x)$$</li>
      <li>Update: $$~~~~~f(x) ~\hookleftarrow~ y$$</li>
    </ul>







    <h2>Monte Carlo Learning</h2>
    <br>


    <p>Value of state is average return.</p>
  
    <br>
    <p>$$V(s_t) ~\hookleftarrow~ R_t~~~~~~~~~~~~$$</p>
    <br>
    <p>$$V(s_t) ~\hookleftarrow~ \sum_i \gamma^i r_{t+i}$$</p>







    <h2>Intersecting Paths: TD Learning</h2>
    <br>
    
    <figure id="figure-mc-vs-td" class="subgrid" style="margin-top: 0;">
      <center>
        <img style="width: 80%; margin-top: 15px;" src="mockups/mc-vs-td.png"/>
      </center>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">4</a>:
        Monte Carlo Learning vs. Temporal Difference Learning.
      </figcaption>
    </figure>

    <br>
    <p>$$V(s_t) ~\hookleftarrow~ r_t + \gamma V(s_{t+1})$$</p>
  

    
    <figure class="l-page">
    <div id="compare2" style="width:650px; height:500px; margin:auto; position:relative;"> </div>
    </figure>
    <script> compare_vis(d3.select("#compare2"), {algs: ["MC", "TD"] }); </script>







    <h2>Re-weighting the Future</h2>
    <br>
    <p>State-Action Values</p>
    <p>Easier to make policies</p>
    <p>MC and TD still work.</p>
    <p>“Q functions are split value state value functions.” How do we merge them back </p>

    <p>Why Q-learning is overconfident</p>
    <br>






    <h2>TD Learning and State Approximators</h2>
    <br>
    <p>Neural networks, so forth</p>

    <img style="width: 100%; margin-top: 15px" src="mockups/averagers.png"/>






    <h2>Playground</h2>
    <br>
    <div id="playground"></div>
    <script src="vis/Playground.js" charset="utf-8"></script>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
