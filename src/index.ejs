<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style type="text/css">@import "vis/playground.css";</style>
  
  <script src="vis/js/d3.min.js" charset="utf-8"></script>
  <script src="vis/js/underscore.js" charset="utf-8"></script>
  <script src="vis/js/rounding.js" charset="utf-8"></script>
  <script src="vis/util.js" charset="utf-8"></script>
  <script src="vis/GridWorld.js" charset="utf-8"></script>
  <script src="vis/LearnQV.js" charset="utf-8"></script>

  <script src="vis/compare.js" charset="utf-8"></script>

</head>


<body>

<d-front-matter>
  <script type="text/json">{
  "title": "The Paths Perspective on Value Learning",
  "description": "",
  "password": "td",
  "authors": [

    {
      "author": "Sam Greydanus<d-footnote>Author order not finalized.</d-footnote>",
      "authorURL": "https://greydanus.github.io/about.html",
      "affiliation": "Google Brain",
      "affiliationURL": "https://g.co/brain"
    },

    {
      "author": "Justin Gilmer",
      "authorURL": "https://scholar.google.com/citations?user=Ml_vQ8MAAAAJ&hl=en",
      "affiliation": "Google Brain",
      "affiliationURL": "https://g.co/brain"
    },

    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "OpenAI",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "strict": false,
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<style>
  #cliff_iter .cell rect {
    fill: #e7eae7;
  }
  .cell rect {
    fill: #e7eae7;
  }
</style>

<d-title>
  <h1 style="grid-column:page;">The Paths Perspective on Value Learning</h1>
  <p  style="grid-column:page;padding-top: 10px;">
    A closer look at how Temporal Difference learning merges paths of experience for greater statistical efficiency.
  </p>
  <style>
    #compare-caption {
      grid-column: kicker;
      border-top: solid 1px rgba(0, 0, 0, 0.1);
      padding-top: 30px;
      margin-top: 10px;
    }
    #compare-figure {
      grid-column:page;
      position:relative;
      margin: 0;
      border-top: solid 1px rgba(0, 0, 0, 0.1);
      padding-top: 30px;
      margin-top: 10px;
    }
    #compare_play_button {
      margin-left: 20px;
      padding: 0 8px;
      background-color: rgba(255, 130, 0);
      border-radius: 4px;
      border: none;
      color: white;
      height: 32px;
      font-size: 13px;
      text-transform: uppercase;
      width: 100px;
    }
    #compare_play_button[disabled] {
      opacity: 0.5;
    }
  </style>
  <div id="compare-caption">

    <button id="compare_play_button" type="button" style="margin: 0 0 20px;">Play</button>
    <div class="figcaption">
      These value estimators behave differently where paths of experience intersect.
    </div>
  </div>
  <figure id="compare-figure" style="grid-column: text-start/page-end;">

    <div id="compare_hero" style="height:400px; position:relative;">
    </div>
    <script> 
      (function() {
        let compare_play_button = document.querySelector("#compare_play_button");

        let run = compare_vis(
          d3.select("#compare_hero"), 
          {algs: ["MC", "TD", "Q"], trail: true}, 
          () => {
            compare_play_button.textContent = "Play";
            compare_play_button.disabled = false;

          }
        );

        compare_play_button.addEventListener("click", () => {

          compare_play_button.textContent = "Playing..."
          compare_play_button.disabled = true;

          run();
        });

        compare_play_button.click();

      })();
      

    </script> 

  </figure>

</d-title>

<d-article>
<!-- <d-cite key="Tsividis2017HumanAtari"></d-cite> -->
  
  <h2>Introduction</h2>
  <p>
    In the last few years, reinforcement learning (RL) has made remarkable progress, including <a href="https://deepmind.com/research/alphago/">beating world-champion Go players</a>, <a href="https://blog.openai.com/learning-dexterity/">controlling robotic hands</a>, and even <a href="https://deepmind.com/blog/learning-to-generate-images/">painting pictures</a>.
  </p>

  <p>
    One of the key sub-problems of RL is value estimation â€“ learning the long-term consequences of being in a state.

    This can be tricky because future returns are generally noisy, affected by many things other than the present state. The further we look into the future, the more this becomes true.

    But while difficult, estimating value is also essential to most approaches to RL.<d-footnote>For many approaches (policy-value iteration), estimating value essentially is the whole problem, while in other approaches (actor-critic models), value estimation is essential for reducing noise.</d-footnote>
  </p>

  <p>
    The natural way to estimate the value of a state is as the average return you observe from that state. We call this Monte Carlo value estimation. 
  </p>

  <figcaption style="grid-column: kicker">
    <b>Cliff World</b> <d-cite key="Sutton2017ReinforcementIntroduction"></d-cite> is a classic RL example, where the agent learns to walk along a <span style="background: #FFD5D5">cliff</span> to reach a <span style="background: #D5D5FF">goal</span>.
  </figcaption>

  <style>
  #cliffworld-mc {
    display: grid;
    grid-template-columns: 1fr 1fr 1fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="cliffworld-mc" class="l-body">
      <img src="figures/cliffworld-path1.svg"/>
      <div class="figcaption">
        Sometimes the agent reaches its goal.
      </div>

      <img src="figures/cliffworld-path2.svg"/>
      <div class="figcaption">
        Other times it falls off the cliff.
      </div>

      <img src="figures/cliffworld-mc.svg"/>
      <div class="figcaption">
        Monte Carlo averages over trajectories where they intersect.
      </div>
  </figure>

  <p>
    If a state is visited by only one episode, Monte Carlo says its value is the return of that episode. If multiple episodes visit a state, Monte Carlo estimates its value to be the average over them.
  </p>

  <p>
    Let's write Monte Carlo a bit more formally.
    In RL, we often describe algorithms with update rules, which tell us how estimates change with one more episode.
    We'll use an "updates toward" ($\hookleftarrow$) operator to keep equations simple.<d-footnote>Details about this notation.</d-footnote>
  </p>

  <div class="eq-grid" style="grid-gap: 8px;">
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>R_t</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:200px;"> Return </figcaption>
  </div>

  <p>
    Estimating value by updating towards return makes a lot of sense. After all, the <i>definition</i> of value is expected return. It might be surprising, then, that we can do better.
  </p>

  <h2>Beating Monte Carlo</h2>

    <p>
      But we <i>can</i> do better! The trick is to use a method called <i>Temporal Difference (TD) learning</i>, which bootstraps off of nearby states to make value updates.
    </p>


    <div class="eq-grid" style="grid-gap: 8px; margin-top: 0; margin-bottom: 0;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>r_{t} </d-math></div>
      <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma V(s_{t+1})</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Reward</figcaption>
      <figcaption style="grid-row: 2; grid-column: 9; max-width:140px;">Next state value</figcaption>

    </div>

    <p>
      Intersections between two trajectories are handled differently under this update. Unlike Monte Carlo, TD updates merge intersections so that the return flows backwards to all preceding states.
    <p>

  <style>
  #cliffworld-td {
    display: grid;
    grid-template-columns: 1fr 1fr 1fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="cliffworld-td" class="l-body">
      <img src="figures/cliffworld-path1.svg"/>
      <div class="figcaption">
        Sometimes the agent reaches its goal.
      </div>

      <img src="figures/cliffworld-path2.svg"/>
      <div class="figcaption">
        Other times it falls off the cliff.
      </div>

      <img src="figures/cliffworld-td.svg"/>
      <div class="figcaption">
        TD learning merges paths where they intersect.
      </div>
  </figure>

    <p>
      What does it mean to "merge trajectories" in a more formal sense? Why might it be a good idea? One thing to notice is that $V(s_{t+1})$ can be written as the expectation over all of its TD updates:
    <p>


    <div class="eq-grid" style="grid-gap: 8px;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_{t+1})~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\simeq~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>\mathop{\mathbb{E}} \bigr[ r'_{t+1} ~+~ \gamma V(s'_{t+2}) \bigl] </d-math></div>

      <div style="grid-row: 3; grid-column: 3;"><d-math>\simeq~~</d-math></div>
      <div style="grid-row: 3; grid-column: 5;"><d-math>\mathop{\mathbb{E}} \bigr[ r'_{t+1} \bigl] ~~+~~ \gamma \mathop{\mathbb{E}} \bigr[ V(s'_{t+2}) \bigl] </d-math></div>

    </div>

    <p>
      We can use this to expand the TD update rule recursively:
    <p>

  <div class="eq-grid" style="grid-gap: 8px;">
  
    <div style="grid-row: 1; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 1; grid-column: 5;"><d-math>r_{t} </d-math></div>
    <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma V(s_{t+1})</d-math></div>

    <div class="expansion-marker" style="grid-row: 2; grid-column: 9 / 14; "></div>

    <div style="grid-row: 3; grid-column: 1;"><d-math></d-math></div>
    <div style="grid-row: 3; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 3; grid-column: 5;"><d-math>r_{t} </d-math></div>
    <div style="grid-row: 3; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 3; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 3; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 3; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{E}} \bigr[ V(s''_{t+3}) \bigl]</d-math></div>

    <div class="expansion-marker" style="grid-row: 4; grid-column: 13 / 18; "></div>

    <div style="grid-row: 5; grid-column: 1;"><d-math></d-math></div>
    <div style="grid-row: 5; grid-column: 3;"><d-math>\hookleftarrow~</d-math></div>
    <div style="grid-row: 5; grid-column: 5;"><d-math>r_{t} </d-math></div>
    <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} ~ \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
    <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{EE}}  ~ \bigr[ r''_{t+2} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 15;"><d-math>+~</d-math></div>
    <div style="grid-row: 5; grid-column: 17;"><d-math>...~~</d-math></div>

</div>

    <p>
      This gives us a strange-looking sum of nested expectation values. At first glance, it's not clear how we should compare them to the more simple-looking Monte Carlo update. More importantly, it's not clear that we <i>should</i> compare the two; the updates are so different that it feels a bit like comparing apples to oranges. Indeed, it's easy to think of Monte Carlo and TD learning as two entirely different approaches.
    </p>

    <p>
      But they are not so different after all. Let's rewrite the Monte Carlo update in terms of reward and place it beside the expanded TD update.
    </p>
  
  <div class="eq-grid" style="grid-gap: 8px;">

    <figcaption style="grid-row: 1; grid-column: 1/3; max-width:140px;"><b>MC update</b></figcaption>
    <div style="grid-row: 2; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 2; grid-column: 3;"><d-math>~\hookleftarrow~~</d-math></div>
    <div style="grid-row: 2; grid-column: 5;"><d-math>r_{t}</d-math></div>
    <div style="grid-row: 2; grid-column: 7;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 9;"><d-math>\gamma ~ r_{t+1}</d-math></div>
    <div style="grid-row: 2; grid-column: 11;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 13;"><d-math>\gamma^2 ~ r_{t+2}</d-math></div>
    <div style="grid-row: 2; grid-column: 15;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>


    <figcaption style="grid-row: 3; grid-column: 5/7; max-width:90px; margin: 0px; padding: 0px;">Reward from present path.</figcaption>

    <figcaption style="grid-row: 3; grid-column: 9/11; max-width:125px; margin: 0px; padding: 0px;">Reward from present path.</figcaption>

    <figcaption style="grid-row: 3; grid-column: 13/14; max-width:125px; margin: 0px; padding: 0px;">Reward from present path...</figcaption>

    <figcaption style="grid-row: 4; grid-column: 1/3; max-width:140px; margin-top: 22px;"><b>TD update</b></figcaption>
    <div style="grid-row: 5; grid-column: 1;"><d-math>V(s_t)~</d-math></div>
    <div style="grid-row: 5; grid-column: 3;"><d-math>~\hookleftarrow~~</d-math></div>
    <div style="grid-row: 5; grid-column: 5;"><d-math>r_{t}</d-math></div>
    <div style="grid-row: 5; grid-column: 7;"><d-math>+~</d-math></div>
    <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} ~ \bigr[ r'_{t+1} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 11;"><d-math>+~</d-math></div>
    <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{EE}} ~ \bigr[ r''_{t+2} \bigl]</d-math></div>
    <div style="grid-row: 5; grid-column: 15;"><d-math>+~</d-math></div>
    <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

    <figcaption style="grid-row: 6; grid-column: 5/7; max-width:90px; margin: 0px; padding: 0px;">Reward from present path.</figcaption>

    <figcaption style="grid-row: 6; grid-column: 9/11; max-width:125px; margin: 0px; padding: 0px;">Expectation over paths intersecting present path.</figcaption>

    <figcaption style="grid-row: 6; grid-column: 13/14; max-width:125px; margin: 0px; padding: 0px;">Expectation over paths intersecting <i>paths intersecting</i> present path...</figcaption>
  </div>


    <p>
      A pleasant correspondence has emerged. The difference between Monte Carlo and TD learning comes down to the nested expectation operators. It turns out that there is a nice visual interpretation for what they are doing. We call it the <i>paths perspective</i> on value learning.
    </p>

    <h2>The Paths Perspective</h2>

    <p>
      We often think about an agent's experience as a series of trajectories. The grouping is logical and easy to visualize.
    </p>

  <style>
  #cliffworld-trajectories {
    margin-left: auto;
    margin-right: auto;
    width: 420px;
    display: grid;
    grid-template-columns: 1fr 1fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="cliffworld-trajectories" class="l-body">
      <img src="figures/cliffworld-path-1of4.svg"/>
      <div class="figcaption">
        Trajectory 1
      </div>

      <img src="figures/cliffworld-path-2of4.svg"/>
      <div class="figcaption">
        Trajectory 2
      </div>
  </figure>


    <p>
      But this way of organizing experience de-emphasizes relationships <i>between</i> trajectories. Wherever two trajectories intersect, both outcomes are valid futures for the agent. So even if the agent has followed Trajectory 1 to the intersection, it could <i>in theory</i> follow Trajectory 2 from that point onward. We can dramatically expand the agent's experience using these simulated trajectories or "paths."
    </p>

    <style>
  #cliffworld-paths {
    margin-left: auto;
    margin-right: auto;
    width: 840px;
    display: grid;
    grid-template-columns: 1fr 1fr 1fr 1fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="cliffworld-paths" class="l-page">
      <img src="figures/cliffworld-path-1of4.svg"/>
      <div class="figcaption">
        Path 1
      </div>

      <img src="figures/cliffworld-path-2of4.svg"/>
      <div class="figcaption">
        Path 2
      </div>

      <img src="figures/cliffworld-path-3of4.svg"/>
      <div class="figcaption">
        Path 3
      </div>

      <img src="figures/cliffworld-path-4of4.svg"/>
      <div class="figcaption">
        Path 4
      </div>
  </figure>

    <p>
      <b>Estimating value.</b> It turns out that Monte Carlo is averaging over real trajectories whereas TD learning is averaging over paths. The nested expectation values we saw earlier correspond to the agent averaging across <i>all possible future paths</i>.
    </p>

  <style>
  #compare-mctd {
    margin-left: auto;
    margin-right: auto;
    width: 800px;
    display: grid;
    grid-template-columns: .955fr 2fr .4fr 2fr 2fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="compare-mctd" class="l-page">
      <img src="figures/traj-thumbnails.svg"/>
      <div class="figcaption">
        Trajectories
      </div>

      <img src="figures/cliffworld-mc.svg"/>
      <div class="figcaption">
        MC estimate
      </div>

      <div></div>
      <div></div>

      <img src="figures/path-thumbnails.svg"/>
      <div class="figcaption">
        Paths
      </div>

      <img src="figures/cliffworld-td.svg"/>
      <div class="figcaption">
        TD estimate
      </div>
  </figure>

    <p>
      <b>Comparing the two.</b> Generally speaking, the best value estimate is the one with the lowest variance. Since both TD and Monte Carlo are empirical averages,<d-footnote>In the tabular case.</d-footnote> the method that gives the better estimate is the one that averages over more items. This raises a natural question: Which estimator averages over more items?
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>Var[V(s)]~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\propto~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>\frac{1}{N} </d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> Variance of estimate </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Inverse of the number of items in the average</figcaption>

    </div>

    <p>
      First off, TD learning never averages over <i>fewer</i> trajectories than Monte Carlo because there are never fewer simulated trajectories (paths) than real trajectories. On the other hand, when there are more simulated trajectories (paths) than real trajectories, TD learning has the chance to average over more of the agent's experience. Given this intuition, it seems plausible that TD estimates will have lower variance.
    </p>

    <!-- <p>
      <b>Analyzing variance.</b> It's possible to express this idea more formally by writing out the variances of the Monte Carlo and TD estimates. We show how to do this in the <a href="#variance-analysis-appendix">Appendix</a>, but for now, we'll simply show the final equations. Note that we've introduced some new notation, $T(s)$, which counts the number of trajectories that contain state $s$.
    </p> -->

    <!-- <div class="eq-grid" style="grid-gap: 8px;">

      <figcaption style="grid-row: 1; grid-column: 1; max-width:140px;"><b>MC variance</b></figcaption>

      <div style="grid-row: 2; grid-column: 1;"><d-math>Var[ V(s_t)]~</d-math></div>
      <div style="grid-row: 2; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 2; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 2; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 9;"><d-math>~\frac{\gamma}{T(s_{t+1})}</d-math></div>
      <div style="grid-row: 2; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 13;"><d-math>~~\frac{\gamma^2}{T(s_{t+2})}</d-math></div>
      <div style="grid-row: 2; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>


      <figcaption style="grid-row: 4; grid-column: 5;"><d-math>~~~~=</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 9;"><d-math>~~~~~~~~\geq</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 13;"><d-math>~~~~~~~~~\geq</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 17;"><d-math>\geq</d-math></figcaption>

      <figcaption style="grid-row: 6; grid-column: 0; max-width:140px;"><b>TD variance</b></figcaption>

      <div style="grid-row: 7; grid-column: 1;"><d-math>Var[V(s_t)]~~</d-math></div>
      <div style="grid-row: 7; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 7; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 7; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 7; grid-column: 9;"><d-math> \frac{\gamma}{ \mathop{\mathbb{E}} [ T(s'_{t+1}) ] }</d-math></div>
      <div style="grid-row: 7; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 7; grid-column: 13;"><d-math>\frac{\gamma^2 }{\mathop{\mathbb{EE}} [T(s''_{t+2})]}</d-math></div>
      <div style="grid-row: 7; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 7; grid-column: 17;"><d-math>...</d-math></div>
  </div> -->

<!--     <p>
      Since every term in the first sum is greater than or equal to the corresponding term in the second sum, tabular<d-footnote>It's important to note that this inequality only holds for tabular value estimators. Later on we'll see that things change when we add function approximation to the mix.</d-footnote> TD estimates will always have equal or lower variance than Monte Carlo.
    </p> -->


    <h2>Re-weighting paths with $Q$-functions</h2>
    
    <p>
      An alternative to the value function is the $Q$-function. One motivation for $Q$-functions is that they make it easier to compare actions:
    </p>

  <style>
  #qlearning-intro {
    margin-left: auto;
    margin-right: auto;
    width: 550px;
    display: grid;
    grid-template-columns: 1fr 1fr 1fr;
    grid-template-rows: auto auto;
    grid-column-gap: 30px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="qlearning-intro" class="l-page">
      <img src="figures/policy.svg"/>
      <div class="figcaption">
        Many times weâ€™d like to compare the value of actions under a policy.
      </div>

      <img src="figures/value.svg"/>
      <div class="figcaption">
        Itâ€™s hard to do this with a value function.
      </div>

      <img src="figures/qvalue.svg"/>
      <div class="figcaption">
        Itâ€™s easier to use $Q$-functions which estimate joint state-action values.
      </div>
  </figure>

    <p>
      We'll see some other nice properties of $Q$-functions in this section. First, though, we need to modify our Monte Carlo and TD updates.
    </p>

    <p>
      <b>Monte Carlo for $Q$-functions.</b> The Monte Carlo update rule looks nearly identical to the one we wrote down for $V(s)$:
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">
      <div style="grid-row: 1; grid-column: 1;"><d-math>Q(s_t, a_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>R_t</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State-action value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:200px;"> Return </figcaption>
    </div>

    <p>
      We still update towards the return. Instead of updating towards the return of being in some state, though, we now update towards the return of being in some state <i>and selecting some action</i>.
    </p>



    <p>
      <b>TD learning and $Q$-functions.</b> We can try adapting the TD learning rule in the same manner:
    </p>

    <div class="eq-grid" style="grid-gap: 8px; margin-top: 0; margin-bottom: 0;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>Q(s_t, a_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>r_{t} </d-math></div>
      <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma Q(s_{t+1}, a_{t+1})</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State-action value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Reward</figcaption>
      <figcaption style="grid-row: 2; grid-column: 9; max-width:140px;">Next state value</figcaption>

    </div>

    <p>
      This version of the TD update rule requires a tuple of the form $(s_t, a_t, r_{t}, s_{t+1}, a_{t+1})$, so we call it the <i>Sarsa</i> algorithm.
      Sarsa may be the simplest way to write a $Q$-function TD update, but it's not the most efficient.
    </p>

    <p>
      <b>Expected Sarsa.</b> The problem with Sarsa is that it uses $Q(s_{t+1},a_{t+1})$ to approximate the value of the next state, when it should be using $V(s_{t+1})$.
    </p>

        <div class="eq-grid" style="grid-gap: 8px; margin-top: 0; margin-bottom: 0;">
    
      <div style="grid-row: 1; grid-column: 1;"><d-math>Q(s_t, a_t)~~</d-math></div>
      <div style="grid-row: 1; grid-column: 3;"><d-math>\hookleftarrow~~</d-math></div>
      <div style="grid-row: 1; grid-column: 5;"><d-math>r_{t} </d-math></div>
      <div style="grid-row: 1; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 1; grid-column: 9;"><d-math>\gamma Q(s_{t+1}, a_{t+1})</d-math></div>

      <figcaption style="grid-row: 2; grid-column: 1; max-width:140px;"> State-action value </figcaption>
      <figcaption style="grid-row: 2; grid-column: 5; max-width:140px;">Reward</figcaption>
      <figcaption style="grid-row: 2; grid-column: 9/10; max-width:160px;">This should be $\gamma V(s_{t+1})$</figcaption>

    </div>

    <p>
     A better way of estimating the next state value is to instead calculate a weighted sum or expectation value over the state's $Q$ values. This approach is called <i>Expected Sarsa</i>.
    </p>

      <style>
      #expected-sarsa {
        width: 700px;
        display: grid;
        grid-template-columns: .4fr 1fr;
        grid-template-rows: auto auto;
        grid-row-gap: 30px;
        grid-column-gap: 30px;
        grid-auto-flow: row;
      }
    </style>
    <figure id="expected-sarsa" class="l-body">
      <div class="figcaption">
        <b>Sarsa</b> uses the $Q$-value associated with $a_{t+1}$ to estimate the next state's value.
      </div>
      <img src="figures/sarsa.svg"/>
      <div class="figcaption">
        <b>Expected Sarsa</b> uses an expectation over $Q$ values to estimate the next state's values.
      </div>
      <img src="figures/expected-sarsa.svg"/>
    </figure>

    <p>
      Here's a surprising fact about Expected Sarsa: the value estimate it gives is often <i>better</i> than a value estimate computed straight from the experience. This is because the expectation value weights the $Q$-values by the true policy distribution rather than the empirical policy distribution. In doing this, Expected Sarsa <i>corrects for the difference between the empirical policy distribution and the true policy distribution.</i>
    </p>

    <p>
      <b>Off-policy value estimation.</b> We can push this idea even further. Instead of weighting $Q$-values by the true policy distribution, we can instead weight them by an arbitrary policy $\pi^{off}$
    </p>

    <style>
      #off-policy {
        width: 700px;
        display: grid;
        grid-template-columns: .4fr 1fr;
        grid-template-rows: auto auto;
        grid-row-gap: 30px;
        grid-column-gap: 30px;
        grid-auto-flow: row;
      }
    </style>
    <figure id="off-policy" class="l-body">
      <div class="figcaption">
        <b>Off-policy value learning</b> weights $Q$ values by an arbitrary policy.
      </div>
      <img src="figures/off-policy.svg"/>
    </figure>

    <p>
      With this slight modification, we can estimate value under any policy we like. We can also see that Expected Sarsa is just a special case of off-policy learning where the "off" policy is the true policy.
    </p>

    <p>
      We can even think of regular Sarsa as a type of off policy learning where the weighting is determined by $a_{t+1}$.
    </p>

    <p>
      <b>Re-weighting path intersections.</b> What does the paths perspective say about off-policy learning? To answer this question, let's consider some state where multiple paths of experience intersect.
    </p>

  <style>
  #reweighting-1 {
    margin-left: auto;
    margin-right: auto;
    width: 130px;
    padding: 0px;
    margin-top: 0px;
    margin-bottom: 20px;
  }
  </style>
  <figure id="reweighting-1" class="l-page">
      <img src="figures/reweighting-1.svg"/>
  </figure>

    <p>
      When these paths exit the state through different actions, they get associated with different $Q$-values.
    </p>

  <style>
  #reweighting-2 {
    margin-left: auto;
    margin-right: auto;
    width: 130px;
    padding: 0px;
    margin-top: 0px;
    margin-bottom: 20px;
  }
  </style>
  <figure id="reweighting-2" class="l-page">
      <img src="figures/reweighting-2.svg"/>
  </figure>

    <p>
      So whenever we re-weight $Q$ values, we are also re-weighting the paths of experience that pass through them. In other words, off-policy learning <i>re-weights path intersections.</i>
    </p>

















  <style>
  #reweighting-full {
    width: 650px;
    margin-top: 20px;
    margin-bottom: 30px;
  }
  </style>
 <!--  <figure id="reweighting-full" class="l-page">
    <embed type="image/svg+xml" src="figures/reweighting-full.svg" />
  </figure> -->

  <div id="reweighting-full" class="l-body">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1196.70082 300.74613">
      <defs>
        <style>
          .cls-1 {
            fill: #263b96;
          }

          .cls-2 {
            fill: #e7ebe8;
          }

          .cls-3 {
            fill: #cac9cc;
          }

          .cls-4 {
            fill: #bb7566;
          }

          .cls-5 {
            font-size: 24.25341px;
            fill: #d1d3d4;
            font-family: MyriadPro-Semibold, Myriad Pro;
            font-weight: 700;
          }

          .cls-6 {
            fill: #6d6e71;
          }

          .cls-7 {
            fill: #010101;
          }

          .cls-10, .cls-11, .cls-12, .cls-13, .cls-14, .cls-8, .cls-9 {
            fill: none;
            stroke-linecap: round;
            stroke-linejoin: round;
          }

          .cls-8, .cls-9 {
            stroke: #9ba2c4;
          }

          .cls-10, .cls-12, .cls-8 {
            stroke-width: 7px;
          }

          .cls-11, .cls-13, .cls-9 {
            stroke-width: 12px;
          }

          .cls-10, .cls-11 {
            stroke: #bb7566;
          }

          .cls-12, .cls-13 {
            stroke: #263b96;
          }

          .cls-14 {
            stroke: #fbb040;
            stroke-width: 4px;
          }
        </style>
      </defs>
      <title>reweighting-full</title>
      <g id="background">
        <rect class="cls-1" x="182.22406" y="8.58848" width="47.96635" height="47.96635"/>
        <rect class="cls-2" x="0.27231" y="4.45507" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="59.67011" y="4.51524" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="118.83064" y="4.52698" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="0.27231" y="63.86432" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="59.67011" y="63.92448" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="118.83064" y="63.93623" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="178.15082" y="63.95972" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="0.27231" y="123.01822" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="59.67011" y="123.07838" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="118.83064" y="123.09012" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="178.15082" y="123.11362" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="0.27231" y="182.42746" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="59.67011" y="182.48763" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="118.83064" y="182.49937" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="178.15082" y="182.52287" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="0.27231" y="241.78314" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="59.67011" y="241.8433" width="56.11284" height="56.11284"/>
        <rect class="cls-2" x="118.83064" y="241.85504" width="56.11285" height="56.11281"/>
        <rect class="cls-2" x="178.15082" y="241.87854" width="56.11284" height="56.11284"/>
        <circle class="cls-3" cx="28.32873" cy="32.51149" r="17.77298"/>
        <circle class="cls-3" cx="87.72653" cy="32.59517" r="17.77298"/>
        <circle class="cls-3" cx="206.15075" cy="151.73133" r="17.77298"/>
        <circle class="cls-3" cx="27.97244" cy="210.69004" r="17.77298"/>
        <circle class="cls-3" cx="87.37025" cy="210.48388" r="17.77298"/>
        <circle class="cls-3" cx="206.15075" cy="210.63648" r="17.77298"/>
        <circle class="cls-3" cx="27.97244" cy="270.50063" r="17.77298"/>
        <circle class="cls-3" cx="87.37025" cy="271.75706" r="17.77298"/>
        <circle class="cls-3" cx="206.15075" cy="270.44707" r="17.77298"/>
        <rect class="cls-4" x="238.82374" y="6.58037" width="55.147" height="290.43915"/>
        <text class="cls-5" transform="translate(248.99794 101.10636)">-1</text>
        <circle class="cls-3" cx="27.90284" cy="91.46093" r="17.77298"/>
        <rect class="cls-2" x="405.1621" y="114.66201" width="125.16666" height="125.16666"/>
        <rect class="cls-2" x="708.45513" y="110.0178" width="123.8606" height="123.8606"/>
        <polygon class="cls-3" points="724.757 171.946 737.582 197.695 737.582 146.196 724.757 171.946"/>
        <polygon class="cls-3" points="770.261 217.897 796.01 205.072 744.511 205.072 770.261 217.897"/>
        <rect class="cls-2" x="1009.34561" y="110.0178" width="123.8606" height="123.8606"/>
        <polygon class="cls-3" points="1025.648 171.946 1038.473 197.695 1038.473 146.196 1025.648 171.946"/>
        <polygon class="cls-3" points="1071.151 217.897 1096.901 205.072 1045.402 205.072 1071.151 217.897"/>
        <polygon class="cls-6" points="1116.904 171.974 1104.079 146.224 1104.079 197.724 1116.904 171.974"/>
        <polygon class="cls-6" points="1071.151 126.265 1045.402 139.09 1096.901 139.09 1071.151 126.265"/>
        <g>
          <path id="MJMATHI-3C0" class="cls-7" d="M1004.86905,30.2902a1.01886,1.01886,0,0,1-1.1696-1.1352v-.3784l.4472-.9632a66.65876,66.65876,0,0,0,3.74959-9.39118l.2752-.8256h-1.1008a11.46871,11.46871,0,0,0-1.85759.1032,3.72159,3.72159,0,0,0-1.3416.6536,4.49326,4.49326,0,0,0-.7568.6192,8.86528,8.86528,0,0,0-.6536.7224,4.23584,4.23584,0,0,0-.3096.4128,1.96159,1.96159,0,0,1-.516.0344q-.65359,0-.6536-.344a4.15339,4.15339,0,0,1,.6536-1.204,11.69755,11.69755,0,0,1,1.892-2.1328,4.67672,4.67672,0,0,1,2.44239-1.30719q.24081-.06881,7.74-.0688,5.504,0,5.64159.0344a.93916.93916,0,0,1,.688.96319,1.43671,1.43671,0,0,1-1.1008,1.4448,18.65553,18.65553,0,0,1-2.37359.0688h-2.2016l-.1032.5848a25.82,25.82,0,0,0-.4128,4.09359,14.98872,14.98872,0,0,0,.3096,3.1992,12.42959,12.42959,0,0,0,.6536,2.2016,4.01153,4.01153,0,0,1,.344.96319,1.59419,1.59419,0,0,1-.4816,1.1008,1.61806,1.61806,0,0,1-1.2384.516,1.58533,1.58533,0,0,1-.6192-.1032q-.2408-.1032-.5504-.8256a9.93709,9.93709,0,0,1-.55039-2.064q-.03441-.3096-.0344-1.5136a22.8307,22.8307,0,0,1,.30959-3.612q.3096-1.92639.6192-3.16479l.344-1.376h-3.37119l-.0344.1376q0,.1032-.6536,2.7176t-1.4792,5.53839a25.634,25.634,0,0,1-1.0664,3.33679A1.44642,1.44642,0,0,1,1004.86905,30.2902Z"/>
          <path id="MJMAIN-28" class="cls-7" d="M1023.273,21.31182a28.29359,28.29359,0,0,1,.344-4.5064,31.52785,31.52785,0,0,1,.79119-3.68079,14.40509,14.40509,0,0,1,1.2728-3.0272q.8256-1.47919,1.3072-2.30479a9.13694,9.13694,0,0,1,1.4448-1.7888q.9632-.96319,1.1352-1.1696a6.199,6.199,0,0,1,.86-.7224h.58479q.48161,0,.4816.3096a2.77951,2.77951,0,0,1-.5848.7224,15.61818,15.61818,0,0,0-1.41039,1.8232,23.7992,23.7992,0,0,0-1.6856,2.95839,19.5707,19.5707,0,0,0-1.4448,4.7472,34.72008,34.72008,0,0,0-.5848,6.63919,34.18012,34.18012,0,0,0,.5848,6.60478,22.34139,22.34139,0,0,0,1.4104,4.7816,15.5792,15.5792,0,0,0,1.6856,2.95839q.86,1.1352,1.44479,1.8232a7.76935,7.76935,0,0,1,.5848.7224q0,.3096-.516.3096h-.55039l-.9632-.8256a16.97617,16.97617,0,0,1-4.7128-7.29279A28.12545,28.12545,0,0,1,1023.273,21.31182Z"/>
          <g>
            <path id="MJMATHI-73" class="cls-7" d="M1037.96179,19.97022a5.0979,5.0979,0,0,1,.5504-2.236,5.65952,5.65952,0,0,1,1.9264-2.0984,5.49631,5.49631,0,0,1,3.33679-.92879,4.34619,4.34619,0,0,1,3.096.92879,2.94731,2.94731,0,0,1,.9976,2.064,2.04867,2.04867,0,0,1-.5848,1.6168,2.08416,2.08416,0,0,1-1.3072.5504,1.37407,1.37407,0,0,1-.8256-.2752,1.113,1.113,0,0,1-.4128-.8944,1.561,1.561,0,0,1,.3096-.9632,3.0902,3.0902,0,0,1,.5848-.6192.83249.83249,0,0,1,.4472-.2064.0304.0304,0,0,0,.0344-.0344q0-.1032-.2408-.3096a2.34973,2.34973,0,0,0-.86-.3784,6.08749,6.08749,0,0,0-1.3416-.2064,2.90965,2.90965,0,0,0-3.19919,2.7176,1.2436,1.2436,0,0,0,.6536,1.204,7.13465,7.13465,0,0,0,1.892.5848,7.538,7.538,0,0,1,1.7888.516,3.39973,3.39973,0,0,1,2.2704,3.30239,5.23467,5.23467,0,0,1-.3784,1.9264,7.695,7.695,0,0,1-1.1352,1.92639,4.97627,4.97627,0,0,1-2.1672,1.5136,9.34078,9.34078,0,0,1-3.23359.5848,5.56426,5.56426,0,0,1-3.74959-.9976,3.18258,3.18258,0,0,1-1.1352-2.33919,2.33951,2.33951,0,0,1,.7224-1.9264,2.37526,2.37526,0,0,1,1.51359-.5848,1.65979,1.65979,0,0,1,.9632.3096,1.28064,1.28064,0,0,1,.4816,1.0664,2.05891,2.05891,0,0,1-.6192,1.5136,1.69416,1.69416,0,0,1-1.0664.61919.26384.26384,0,0,1-.1032.0344q-.03439,0-.0344.0688a.74352.74352,0,0,0,.2752.2752,4.80236,4.80236,0,0,0,1.0664.4472,6.76591,6.76591,0,0,0,4.0248-.1376,2.95524,2.95524,0,0,0,1.30719-.96319,4.56666,4.56666,0,0,0,.5848-1.0664,2.59463,2.59463,0,0,0,.172-.86,1.62061,1.62061,0,0,0-.7224-1.4448,3.65668,3.65668,0,0,0-1.65119-.5848,8.51355,8.51355,0,0,1-2.0296-.516,3.18344,3.18344,0,0,1-1.6168-1.204A3.4626,3.4626,0,0,1,1037.96179,19.97022Z"/>
            <g>
              <path id="MJMATHI-74" class="cls-7" d="M1050.22131,25.7083q-.17024-.17025-.17024-.24321a2.045,2.045,0,0,1,.073-.38913.99961.99961,0,0,1,.1216-.34049.2089.2089,0,0,1,.21889-.1216q.17025,0,1.24036-.02433h1.289l.4621-1.94566q.073-.26752.17024-.7053a5.21651,5.21651,0,0,1,.17025-.63234,4.5779,4.5779,0,0,0,.14592-.48641.89222.89222,0,0,1,.19457-.41346,2.314,2.314,0,0,0,.2432-.29185.54809.54809,0,0,1,.3405-.21888,2.0123,2.0123,0,0,1,.43777-.04864.7457.7457,0,0,1,.608.26752.81493.81493,0,0,1,.17024.4621,9.244,9.244,0,0,1-.21888,1.14308q-.2189.97283-.48642,1.92134l-.24321.89986a5.7461,5.7461,0,0,0,1.216.04864h1.24036q.17025.17025.17025.26753a1.15027,1.15027,0,0,1-.31617.85123h-2.60233l-.87554,3.52651q-.94851,3.89133-.94851,4.18317,0,1.02147.65666,1.02147a2.32436,2.32436,0,0,0,1.65381-.8269,6.19441,6.19441,0,0,0,1.289-2.04295q.04865-.14592.1216-.17024a2.14125,2.14125,0,0,1,.36482-.04864h.09728q.36481,0,.36481.19456a1.194,1.194,0,0,1-.073.29185,4.57575,4.57575,0,0,1-.43778.92419,8.64486,8.64486,0,0,1-.8269,1.14308,4.49726,4.49726,0,0,1-1.24036,1.02147,2.97809,2.97809,0,0,1-1.48357.41345,2.81284,2.81284,0,0,1-1.38628-.36481,2.10717,2.10717,0,0,1-.94851-1.26468,3.05762,3.05762,0,0,1-.04864-.65666V32.6154l.85122-3.40491q.85124-3.42922.87555-3.45355a5.97633,5.97633,0,0,0-1.24036-.04864Z"/>
              <path id="MJMAIN-2B" class="cls-7" d="M1059.73073,29.30777V28.9916a.52977.52977,0,0,1,.34049-.48641h7.27191V24.85707l.02432-3.64811q.24321-.31617.4621-.31617a.53179.53179,0,0,1,.48641.36481v7.24759h7.24759a.50668.50668,0,0,1,0,.97283H1068.316V36.7256q-.19456.3405-.43777.3405h-.09729a.47146.47146,0,0,1-.43777-.3405V29.478h-7.27191a.52979.52979,0,0,1-.34049-.48642Z"/>
              <path id="MJMAIN-31" class="cls-7" d="M1082.495,21.01439l-.31617.12161a6.9692,6.9692,0,0,1-.97283.2432,11.21106,11.21106,0,0,1-1.4106.17025h-.46209V20.43069h.46209a7.90887,7.90887,0,0,0,2.11591-.36481,5.74322,5.74322,0,0,0,1.362-.5837,6.23526,6.23526,0,0,0,.681-.535q.04865-.073.29185-.073a.6774.6774,0,0,1,.41345.14592V26.292l.02432,7.29623a.96309.96309,0,0,0,.29185.21888,2.884,2.884,0,0,0,.5837.09729q.46209.04863,1.50789.04864h.63234v1.11875h-.26753q-.51074-.073-3.867-.073-3.30762,0-3.81836.073h-.29185V33.953h1.55653a5.84225,5.84225,0,0,0,.608-.02432,2.38309,2.38309,0,0,0,.38913-.073,1.45771,1.45771,0,0,1,.19457-.04865q.02431,0,.14592-.1216t.14592-.09728Z"/>
            </g>
          </g>
          <path id="MJMAIN-2C" class="cls-7" d="M1095.6161,28.7078v-.86a2.15118,2.15118,0,0,1,.5504-1.47919,1.88022,1.88022,0,0,1,1.4792-.6192,2.24863,2.24863,0,0,1,1.72.86,4.749,4.749,0,0,1,.7912,3.02719,8.85022,8.85022,0,0,1-.3096,2.3392,10.69487,10.69487,0,0,1-.7224,1.96079,7.45985,7.45985,0,0,1-.89439,1.4104q-.4816.58481-.8256.9288t-.4472.3096q-.13759,0-.4472-.3096t-.3096-.4472q0-.13759.3784-.5504t.86-1.032a6.602,6.602,0,0,0,.8944-1.72,9.88149,9.88149,0,0,0,.5504-2.5112v-.4128l-.0344.0344a1.33345,1.33345,0,0,1-.172.0688.86838.86838,0,0,0-.2064.1032.56292.56292,0,0,1-.3096.0688,2.50726,2.50726,0,0,0-.4128.0344,2.048,2.048,0,0,1-1.5136-.5848Z"/>
          <path id="MJMATHI-61" class="cls-7" d="M1109.37608,24.511a10.1656,10.1656,0,0,1,2.61439-6.60479,7.698,7.698,0,0,1,5.88239-3.16479,3.89986,3.89986,0,0,1,3.096,1.68559,1.72127,1.72127,0,0,1,1.5824-1.032,1.23088,1.23088,0,0,1,.7912.2752.84714.84714,0,0,1,.344.688q0,.44721-1.2728,5.504t-1.3072,5.7104a1.84033,1.84033,0,0,0,.2408,1.13519,1.01048,1.01048,0,0,0,.7224.3096,1.49462,1.49462,0,0,0,.688-.3096,7.80016,7.80016,0,0,0,1.41039-3.30239q.2064-.688.344-.7224a1.00632,1.00632,0,0,1,.344-.0344h.1376q.65361,0,.6536.3096a5.4633,5.4633,0,0,1-.172.9288,10.79109,10.79109,0,0,1-.688,1.85759,5.8606,5.8606,0,0,1-1.10079,1.72,3.40167,3.40167,0,0,1-1.1008.7224,3.66725,3.66725,0,0,1-.8256.0688,3.51617,3.51617,0,0,1-1.9608-.516,2.641,2.641,0,0,1-1.032-1.0664l-.2064-.516q-.0344-.03439-.1376.0344l-.1376.1376a6.03111,6.03111,0,0,1-4.128,1.9264,4.69473,4.69473,0,0,1-3.33679-1.376A5.79662,5.79662,0,0,1,1109.37608,24.511Zm10.93918-5.88239a3.11392,3.11392,0,0,0-.172-.7568,3.27061,3.27061,0,0,0-.7912-1.204,2.19893,2.19893,0,0,0-1.58239-.688,3.30114,3.30114,0,0,0-2.3048,1.0664,7.1978,7.1978,0,0,0-1.72,2.7864,27.46341,27.46341,0,0,0-1.4104,5.64159v.3784a1.83339,1.83339,0,0,1-.0344.4128,2.889,2.889,0,0,0,.6192,2.13279,2.21544,2.21544,0,0,0,1.4792.6192,3.88444,3.88444,0,0,0,2.58-1.1352,5.78423,5.78423,0,0,0,1.51359-1.75439q.0688-.13761.9288-3.6808T1120.31526,18.62862Z"/>
          <path id="MJMAIN-29" class="cls-7" d="M1128.53685,4.14624l.1376-.0344h.7568l.96319.8256a16.97608,16.97608,0,0,1,4.7128,7.29279,28.12547,28.12547,0,0,1,1.47919,9.08159,28.71809,28.71809,0,0,1-.344,4.50639,29.57809,29.57809,0,0,1-.7912,3.68079,15.49406,15.49406,0,0,1-1.2728,3.0272q-.8256,1.51359-1.3072,2.30479a10.46426,10.46426,0,0,1-1.4104,1.7544q-.9288.9632-1.1008,1.1352t-.79119.6536l-.1376.1376h-.7912a.20214.20214,0,0,1-.172-.1032.8097.8097,0,0,1-.1032-.3096q.0344-.03441.3784-.4472,5.332-5.53839,5.332-16.34t-5.332-16.34q-.344-.4128-.3784-.4472A.40432.40432,0,0,1,1128.53685,4.14624Z"/>
        </g>
        <g>
          <path id="MJMATHI-3C0-2" data-name="MJMATHI-3C0" class="cls-7" d="M687.14675,31.10813a.96308.96308,0,0,1-1.10556-1.073V29.6774l.42271-.91047a63.0086,63.0086,0,0,0,3.54432-8.877l.26013-.7804h-1.04053a10.84007,10.84007,0,0,0-1.7559.09755,3.51791,3.51791,0,0,0-1.26815.61781,4.24809,4.24809,0,0,0-.71536.5853,8.37809,8.37809,0,0,0-.61782.68285,4.0065,4.0065,0,0,0-.29265.3902,1.85385,1.85385,0,0,1-.48775.03252q-.61782,0-.61781-.32517a3.92559,3.92559,0,0,1,.61781-1.13808,11.0571,11.0571,0,0,1,1.78842-2.016,4.42065,4.42065,0,0,1,2.30868-1.23563q.22761-.065,7.31624-.065,5.20266,0,5.33272.03252a.88775.88775,0,0,1,.65033.91047,1.358,1.358,0,0,1-1.04053,1.36569,17.63311,17.63311,0,0,1-2.24364.065h-2.08107l-.09755.55278a24.4068,24.4068,0,0,0-.3902,3.86948,14.168,14.168,0,0,0,.29265,3.024,11.7484,11.7484,0,0,0,.61782,2.08107,3.792,3.792,0,0,1,.32516.91046,1.50693,1.50693,0,0,1-.45523,1.04054,1.52949,1.52949,0,0,1-1.1706.48774,1.4984,1.4984,0,0,1-.5853-.09755q-.22761-.09755-.52026-.78039a9.39361,9.39361,0,0,1-.52027-1.951q-.03252-.29265-.03251-1.43073a21.57964,21.57964,0,0,1,.29264-3.41425q.29265-1.82092.5853-2.99153l.32517-1.30066h-3.18663l-.03252.13006q0,.09756-.61781,2.56882T689.553,27.04355a24.23268,24.23268,0,0,1-1.008,3.15412A1.36725,1.36725,0,0,1,687.14675,31.10813Z"/>
          <g>
            <path id="MJMATHI-6F" class="cls-7" d="M706.13994,17.60648a3.63774,3.63774,0,0,1-2.7817-1.12647,3.8223,3.8223,0,0,1-1.0575-2.71273,6.0698,6.0698,0,0,1,.68967-2.82768,7.40777,7.40777,0,0,1,1.88512-2.32192,5.66044,5.66044,0,0,1,3.56334-1.40234q.73565,0,.91957.023.29886.069.59772.16092a4.72058,4.72058,0,0,1,.80462.36783,2.62045,2.62045,0,0,1,.82761.68968,4.48193,4.48193,0,0,1,.59772,1.12647,4.42968,4.42968,0,0,1,.27588,1.60925,6.13445,6.13445,0,0,1-1.97708,4.43693A6.04161,6.04161,0,0,1,706.13994,17.60648Zm-1.83914-3.01159a1.91018,1.91018,0,0,0,1.95409,2.161,2.96025,2.96025,0,0,0,1.90811-.73566,5.12835,5.12835,0,0,0,1.42533-1.93109,14.00458,14.00458,0,0,0,.52875-1.70121,8.45478,8.45478,0,0,0,.32185-2.115,2.45958,2.45958,0,0,0-.41381-1.54028,1.7886,1.7886,0,0,0-1.47131-.68968,3.24928,3.24928,0,0,0-2.5518,1.33338,6.559,6.559,0,0,0-1.26441,2.68974A13.11621,13.11621,0,0,0,704.3008,14.59489Z"/>
            <path id="MJMATHI-66" class="cls-7" d="M715.38162,21.07786a.35282.35282,0,0,1,.13794.046.84551.84551,0,0,0,.25288.069,1.99388,1.99388,0,0,0,.27587.023.723.723,0,0,0,.55174-.29886,1.9005,1.9005,0,0,0,.36783-.66668,32.52708,32.52708,0,0,0,.78163-3.51736q.55175-2.89665,1.05751-5.51742t.50576-2.64377v-.069h-1.08049a6.204,6.204,0,0,1-1.14946-.046.26893.26893,0,0,1-.092-.22989l.16092-.6437a.179.179,0,0,1,.18392-.115q.13792,0,1.08049-.023a9.95184,9.95184,0,0,0,1.10349-.023q0-.046.2069-1.14946t.27587-1.33338q.8506-3.79323,3.35643-3.79322a2.32126,2.32126,0,0,1,1.40235.50576,1.50705,1.50705,0,0,1,.55174,1.21843,1.40749,1.40749,0,0,1-.48278,1.19544,1.731,1.731,0,0,1-.96554.39082q-1.01154,0-1.01153-.96555a1.19161,1.19161,0,0,1,.25288-.75865,1.79009,1.79009,0,0,1,.55174-.48277l.20691-.11495a1.97019,1.97019,0,0,0-.68968-.13793.78043.78043,0,0,0-.52876.22989,1.1906,1.1906,0,0,0-.34483.6437q-.16093.66669-.36783,1.79316t-.36783,1.90811q-.16092.78163-.16092.82761a6.56212,6.56212,0,0,0,1.2644.046q1.03452,0,1.19545.023a.3787.3787,0,0,1,.25288.13794,1.07581,1.07581,0,0,1-.046.39081,1.46231,1.46231,0,0,1-.115.36783.19747.19747,0,0,1-.2069.11495q-.16092,0-1.24142.023h-1.2874l-.52875,2.80469q-1.10349,5.70134-1.54028,7.12666a6.782,6.782,0,0,1-1.60925,2.89665,2.59171,2.59171,0,0,1-1.67821.73566,2.20631,2.20631,0,0,1-1.33338-.4368,1.4461,1.4461,0,0,1-.59772-1.24142,1.50408,1.50408,0,0,1,.48277-1.24142,1.53588,1.53588,0,0,1,.96555-.39082q1.01153,0,1.01153.96555a1.19166,1.19166,0,0,1-.25288.75865,1.79012,1.79012,0,0,1-.55175.48277Z"/>
            <path id="MJMATHI-66-2" data-name="MJMATHI-66" class="cls-7" d="M728.0487,21.07786a.35274.35274,0,0,1,.13793.046.84567.84567,0,0,0,.25289.069,1.99388,1.99388,0,0,0,.27587.023.723.723,0,0,0,.55174-.29886,1.9005,1.9005,0,0,0,.36783-.66668,32.52971,32.52971,0,0,0,.78163-3.51736q.55174-2.89665,1.05751-5.51742t.50576-2.64377v-.069h-1.08049a6.20419,6.20419,0,0,1-1.14947-.046.269.269,0,0,1-.09195-.22989l.16092-.6437a.17894.17894,0,0,1,.18392-.115q.13792,0,1.08049-.023a9.95155,9.95155,0,0,0,1.10348-.023q0-.046.20691-1.14946t.27587-1.33338q.85061-3.79323,3.35643-3.79322a2.3212,2.3212,0,0,1,1.40234.50576,1.507,1.507,0,0,1,.55175,1.21843,1.40749,1.40749,0,0,1-.48278,1.19544,1.731,1.731,0,0,1-.96555.39082q-1.01153,0-1.01152-.96555a1.19161,1.19161,0,0,1,.25288-.75865,1.79009,1.79009,0,0,1,.55174-.48277l.2069-.11495a1.9701,1.9701,0,0,0-.68967-.13793.78043.78043,0,0,0-.52876.22989,1.19067,1.19067,0,0,0-.34484.6437q-.16092.66669-.36782,1.79316t-.36783,1.90811q-.16092.78163-.16093.82761a6.5622,6.5622,0,0,0,1.26441.046q1.03452,0,1.19544.023a.3787.3787,0,0,1,.25289.13794,1.07581,1.07581,0,0,1-.046.39081,1.46231,1.46231,0,0,1-.115.36783.19747.19747,0,0,1-.2069.11495q-.16094,0-1.24142.023H733.888l-.52875,2.80469q-1.10349,5.70134-1.54028,7.12666a6.78212,6.78212,0,0,1-1.60925,2.89665,2.59171,2.59171,0,0,1-1.67822.73566,2.2063,2.2063,0,0,1-1.33337-.4368,1.4461,1.4461,0,0,1-.59772-1.24142,1.50408,1.50408,0,0,1,.48277-1.24142,1.53588,1.53588,0,0,1,.96555-.39082q1.01152,0,1.01153.96555a1.19162,1.19162,0,0,1-.25289.75865,1.79,1.79,0,0,1-.55174.48277Z"/>
          </g>
          <path id="MJMAIN-28-2" data-name="MJMAIN-28" class="cls-7" d="M744.311,22.62129a26.7463,26.7463,0,0,1,.32516-4.25968,29.80147,29.80147,0,0,1,.74789-3.47927,13.61638,13.61638,0,0,1,1.20311-2.86147q.7804-1.39821,1.23563-2.17861a8.63618,8.63618,0,0,1,1.3657-1.69086q.91047-.91047,1.073-1.10557a5.86015,5.86015,0,0,1,.81292-.68285h.55278q.45524,0,.45523.29265a2.62748,2.62748,0,0,1-.55278.68285,14.76225,14.76225,0,0,0-1.33318,1.72338,22.49712,22.49712,0,0,0-1.59332,2.79643,18.49955,18.49955,0,0,0-1.3657,4.48729,32.819,32.819,0,0,0-.55278,6.27571,32.30853,32.30853,0,0,0,.55278,6.24319,21.11815,21.11815,0,0,0,1.33319,4.51981,14.72592,14.72592,0,0,0,1.59331,2.79643q.81291,1.07306,1.3657,1.72339a7.34643,7.34643,0,0,1,.55278.68284q0,.29265-.48775.29265h-.52026l-.91047-.78039a16.04673,16.04673,0,0,1-4.45478-6.89353A26.5857,26.5857,0,0,1,744.311,22.62129Z"/>
          <g>
            <path id="MJMATHI-73-2" data-name="MJMATHI-73" class="cls-7" d="M758.16306,21.35314a4.81884,4.81884,0,0,1,.52026-2.11358,5.34968,5.34968,0,0,1,1.82094-1.98351,5.1954,5.1954,0,0,1,3.15411-.87795,4.10817,4.10817,0,0,1,2.92649.87795,2.78592,2.78592,0,0,1,.943,1.951,1.93646,1.93646,0,0,1-.55279,1.52828,1.97,1.97,0,0,1-1.23563.52026,1.29881,1.29881,0,0,1-.7804-.26013,1.052,1.052,0,0,1-.3902-.84543,1.47557,1.47557,0,0,1,.29265-.91047,2.92108,2.92108,0,0,1,.55279-.5853.78694.78694,0,0,1,.42271-.1951.02874.02874,0,0,0,.03252-.03251q0-.09756-.22762-.29265a2.22078,2.22078,0,0,0-.81291-.35768,5.753,5.753,0,0,0-1.26815-.1951,2.75034,2.75034,0,0,0-3.024,2.56881,1.1755,1.1755,0,0,0,.61782,1.13808,6.74343,6.74343,0,0,0,1.78841.55278,7.12614,7.12614,0,0,1,1.69087.48775,3.21362,3.21362,0,0,1,2.14609,3.1216,4.948,4.948,0,0,1-.35768,1.82093,7.27342,7.27342,0,0,1-1.073,1.82093,4.70389,4.70389,0,0,1-2.04854,1.43073,8.82917,8.82917,0,0,1-3.05657.55278,5.25965,5.25965,0,0,1-3.54431-.943,3.00837,3.00837,0,0,1-1.073-2.21113,2.21142,2.21142,0,0,1,.68285-1.82093,2.24524,2.24524,0,0,1,1.43073-.55278,1.56893,1.56893,0,0,1,.91047.29265,1.21055,1.21055,0,0,1,.45523,1.008,1.94618,1.94618,0,0,1-.5853,1.43074,1.60142,1.60142,0,0,1-1.008.58529.24961.24961,0,0,1-.09755.03252q-.03252,0-.03252.065a.70278.70278,0,0,0,.26013.26014,4.53979,4.53979,0,0,0,1.008.42271,6.39545,6.39545,0,0,0,3.80444-.13006,2.79343,2.79343,0,0,0,1.23563-.91047,4.31664,4.31664,0,0,0,.55279-1.008,2.45258,2.45258,0,0,0,.16258-.81292,1.53189,1.53189,0,0,0-.68285-1.3657,3.45649,3.45649,0,0,0-1.5608-.55278,8.04752,8.04752,0,0,1-1.91848-.48775,3.0092,3.0092,0,0,1-1.52828-1.13808A3.27305,3.27305,0,0,1,758.16306,21.35314Z"/>
            <g>
              <path id="MJMATHI-74-2" data-name="MJMATHI-74" class="cls-7" d="M769.7514,26.77708q-.16092-.16092-.16093-.22989a1.93272,1.93272,0,0,1,.069-.36783.94472.94472,0,0,1,.115-.32185.19747.19747,0,0,1,.2069-.115q.16092,0,1.17245-.023h1.21843l.4368-1.83914q.069-.25287.16092-.66668a4.93219,4.93219,0,0,1,.16093-.59773,4.3242,4.3242,0,0,0,.13793-.45978.84344.84344,0,0,1,.18392-.39082,2.18908,2.18908,0,0,0,.22989-.27587.518.518,0,0,1,.32185-.2069,1.90254,1.90254,0,0,1,.41381-.046.70489.70489,0,0,1,.57473.25288.77028.77028,0,0,1,.16092.4368,8.73865,8.73865,0,0,1-.2069,1.08049q-.20691.91958-.45979,1.81615l-.22989.85061a5.43231,5.43231,0,0,0,1.14946.046h1.17246q.16092.16093.16092.25289a1.0873,1.0873,0,0,1-.29886.80462h-2.45985l-.82761,3.33344q-.89658,3.67829-.89658,3.95415,0,.96555.62071.96555a2.19712,2.19712,0,0,0,1.56327-.78163,5.85551,5.85551,0,0,0,1.21843-1.9311q.046-.13794.11494-.16093a2.02524,2.02524,0,0,1,.34484-.046h.092q.34484,0,.34484.18391a1.12856,1.12856,0,0,1-.069.27587,4.325,4.325,0,0,1-.41381.87359,8.17166,8.17166,0,0,1-.78163,1.0805,4.251,4.251,0,0,1-1.17245.96555,2.81505,2.81505,0,0,1-1.40235.39081,2.65883,2.65883,0,0,1-1.31039-.34484,1.9918,1.9918,0,0,1-.89658-1.19544,2.89062,2.89062,0,0,1-.046-.62071V33.306l.80462-3.2185q.80462-3.24149.82761-3.26447a5.6494,5.6494,0,0,0-1.17245-.046Z"/>
              <path id="MJMAIN-2B-2" data-name="MJMAIN-2B" class="cls-7" d="M778.7402,30.17949v-.29886a.50078.50078,0,0,1,.32185-.45979h6.87378V25.97246l.023-3.44839q.22989-.29886.4368-.29886a.50266.50266,0,0,1,.45978.34484v6.85079h6.8508a.47894.47894,0,0,1,0,.91957h-6.8508v6.8508q-.18391.32185-.4138.32185h-.092a.44565.44565,0,0,1-.41381-.32185v-6.8508h-6.87378a.50076.50076,0,0,1-.32185-.45978Z"/>
              <path id="MJMAIN-31-2" data-name="MJMAIN-31" class="cls-7" d="M800.25814,22.34015l-.29886.115a6.58918,6.58918,0,0,1-.91957.22989,10.59672,10.59672,0,0,1-1.33338.16093h-.4368V21.78841h.4368a7.47606,7.47606,0,0,0,2.00007-.34484,5.42907,5.42907,0,0,0,1.28739-.55174,5.89415,5.89415,0,0,0,.6437-.50576q.046-.069.27587-.069a.64033.64033,0,0,1,.39082.13793v6.87379l.023,6.89678a.91035.91035,0,0,0,.27587.2069,2.72623,2.72623,0,0,0,.55174.092q.4368.046,1.42534.046h.59772v1.0575H804.925q-.48278-.069-3.6553-.069-3.12653,0-3.60931.069h-.27587v-1.0575h1.47131a5.52282,5.52282,0,0,0,.57473-.023,2.2516,2.2516,0,0,0,.36783-.069,1.37832,1.37832,0,0,1,.18392-.046q.023,0,.13793-.115t.13794-.09195Z"/>
            </g>
          </g>
          <path id="MJMAIN-2C-2" data-name="MJMAIN-2C" class="cls-7" d="M812.69344,29.61237v-.81292a2.03342,2.03342,0,0,1,.52026-1.39821,1.7773,1.7773,0,0,1,1.39822-.5853,2.12557,2.12557,0,0,1,1.62583.81291,4.4891,4.4891,0,0,1,.74788,2.86147,8.36571,8.36571,0,0,1-.29265,2.21113,10.10985,10.10985,0,0,1-.68285,1.85344,7.05138,7.05138,0,0,1-.84543,1.33318q-.45524.5528-.7804.87795t-.42272.29265q-.13007,0-.42271-.29265t-.29265-.42271q0-.13007.35768-.52027t.81292-.9755a6.24032,6.24032,0,0,0,.84543-1.62583,9.34069,9.34069,0,0,0,.52027-2.37371v-.3902l-.03252.03252a1.26351,1.26351,0,0,1-.16258.065.82045.82045,0,0,0-.1951.09755.53221.53221,0,0,1-.29265.065,2.36932,2.36932,0,0,0-.3902.03252,1.93591,1.93591,0,0,1-1.43073-.55278Z"/>
          <path id="MJMATHI-61-2" data-name="MJMATHI-61" class="cls-7" d="M825.70009,25.64534a9.609,9.609,0,0,1,2.47126-6.24319,7.2765,7.2765,0,0,1,5.56034-2.99153,3.68633,3.68633,0,0,1,2.9265,1.59331,1.627,1.627,0,0,1,1.49576-.9755,1.16338,1.16338,0,0,1,.74788.26014.80074.80074,0,0,1,.32517.65033q0,.42272-1.20311,5.20266t-1.23564,5.39776a1.73965,1.73965,0,0,0,.22762,1.073.9552.9552,0,0,0,.68285.29265,1.41286,1.41286,0,0,0,.65033-.29265,7.37326,7.37326,0,0,0,1.33318-3.1216q.19511-.65032.32517-.68285a.95112.95112,0,0,1,.32517-.03251h.13006q.61782,0,.61782.29264a5.16523,5.16523,0,0,1-.16258.878,10.2007,10.2007,0,0,1-.65034,1.7559,5.53951,5.53951,0,0,1-1.04053,1.62583,3.21516,3.21516,0,0,1-1.04053.68285,3.46641,3.46641,0,0,1-.7804.065,3.32372,3.32372,0,0,1-1.85345-.48774,2.49644,2.49644,0,0,1-.9755-1.008l-.1951-.48775q-.0325-.03252-.13006.03252l-.13007.13006a5.70092,5.70092,0,0,1-3.902,1.82093,4.43773,4.43773,0,0,1-3.15412-1.30066A5.47931,5.47931,0,0,1,825.70009,25.64534ZM836.04037,20.085a2.94345,2.94345,0,0,0-.16258-.71537,3.09155,3.09155,0,0,0-.74788-1.13808,2.07848,2.07848,0,0,0-1.49577-.65033,3.1204,3.1204,0,0,0-2.17861,1.008,6.80378,6.80378,0,0,0-1.62583,2.63385,25.959,25.959,0,0,0-1.33318,5.33272v.35769a1.73315,1.73315,0,0,1-.03252.3902,2.73084,2.73084,0,0,0,.5853,2.016,2.09418,2.09418,0,0,0,1.39821.5853,3.6718,3.6718,0,0,0,2.43875-1.07305,5.46765,5.46765,0,0,0,1.43073-1.65835q.065-.13007.87795-3.47928T836.04037,20.085Z"/>
          <path id="MJMAIN-29-2" data-name="MJMAIN-29" class="cls-7" d="M843.77933,6.3955l.13006-.03252h.71537l.91047.7804A16.04673,16.04673,0,0,1,849.99,14.0369a26.58549,26.58549,0,0,1,1.39822,8.58439,27.14517,27.14517,0,0,1-.32517,4.25968,27.958,27.958,0,0,1-.74788,3.47928,14.64509,14.64509,0,0,1-1.20312,2.86146q-.78039,1.43073-1.23563,2.17862a9.89149,9.89149,0,0,1-1.33318,1.65834q-.87795.91047-1.04053,1.07305t-.74788.61782l-.13007.13006h-.74788a.19107.19107,0,0,1-.16259-.09755.76557.76557,0,0,1-.09755-.29265q.03252-.03251.35769-.42271,5.04008-5.23518,5.04007-15.4454T843.97443,7.1759q-.32517-.39021-.35769-.42272A.38218.38218,0,0,1,843.77933,6.3955Z"/>
        </g>
        <g>
          <path id="MJMATHI-73-3" data-name="MJMATHI-73" class="cls-7" d="M439.75327,17.277a5.687,5.687,0,0,1,.614-2.49437,6.31339,6.31339,0,0,1,2.149-2.34087,6.13145,6.13145,0,0,1,3.72237-1.03613,4.84835,4.84835,0,0,1,3.45375,1.03613,3.28784,3.28784,0,0,1,1.11288,2.3025,2.28536,2.28536,0,0,1-.65238,1.80362,2.32491,2.32491,0,0,1-1.45825.614,1.53278,1.53278,0,0,1-.921-.307,1.24159,1.24159,0,0,1-.4605-.99775,1.74139,1.74139,0,0,1,.34538-1.0745,3.44757,3.44757,0,0,1,.65237-.69075.9287.9287,0,0,1,.49888-.23025.03391.03391,0,0,0,.03837-.03837q0-.11513-.26862-.34538a2.62112,2.62112,0,0,0-.95938-.42212,6.78959,6.78959,0,0,0-1.49662-.23025,3.24586,3.24586,0,0,0-3.56887,3.03162,1.38728,1.38728,0,0,0,.72912,1.34312,7.95831,7.95831,0,0,0,2.11062.65238,8.40959,8.40959,0,0,1,1.9955.57562,3.7926,3.7926,0,0,1,2.53275,3.684,5.83975,5.83975,0,0,1-.42212,2.149,8.58393,8.58393,0,0,1-1.26638,2.149,5.55137,5.55137,0,0,1-2.41762,1.6885,10.4202,10.4202,0,0,1-3.60725.65237q-2.95487,0-4.18287-1.11287a3.5504,3.5504,0,0,1-1.26638-2.6095,2.60984,2.60984,0,0,1,.80588-2.149,2.64978,2.64978,0,0,1,1.6885-.65237,1.8516,1.8516,0,0,1,1.0745.34537,1.42868,1.42868,0,0,1,.53725,1.18963,2.29679,2.29679,0,0,1-.69075,1.68849,1.88991,1.88991,0,0,1-1.18963.69075.29441.29441,0,0,1-.11512.03838q-.03839,0-.03838.07675a.82951.82951,0,0,0,.307.307,5.35747,5.35747,0,0,0,1.18963.49887,7.5476,7.5476,0,0,0,4.48987-.1535,3.29664,3.29664,0,0,0,1.45825-1.0745,5.0939,5.0939,0,0,0,.65237-1.18962,2.89444,2.89444,0,0,0,.19188-.95937,1.80786,1.80786,0,0,0-.80588-1.61175,4.07927,4.07927,0,0,0-1.842-.65238,9.49745,9.49745,0,0,1-2.26413-.57562,3.55133,3.55133,0,0,1-1.80362-1.34313A3.86268,3.86268,0,0,1,439.75327,17.277Z"/>
          <g>
            <path id="MJMATHI-74-3" data-name="MJMATHI-74" class="cls-7" d="M453.42942,23.67818q-.18991-.18991-.18991-.27131a2.28124,2.28124,0,0,1,.08139-.4341,1.11463,1.11463,0,0,1,.13566-.37983.233.233,0,0,1,.24418-.13566q.18992,0,1.38368-.02713h1.438l.51549-2.17049q.08139-.29844.18992-.7868a5.82106,5.82106,0,0,1,.18992-.70541,5.10656,5.10656,0,0,0,.16278-.54262.99549.99549,0,0,1,.217-.46123,2.58274,2.58274,0,0,0,.27131-.32557.61134.61134,0,0,1,.37984-.24418,2.24479,2.24479,0,0,1,.48836-.05427.83185.83185,0,0,1,.67828.29845.909.909,0,0,1,.18992.51549,10.31341,10.31341,0,0,1-.24418,1.27516q-.24418,1.08525-.54263,2.14336l-.27131,1.00385a6.41079,6.41079,0,0,0,1.35656.05426h1.38368q.18993.18991.18992.29844a1.28319,1.28319,0,0,1-.3527.94959h-2.903l-.97672,3.934q-1.05812,4.341-1.05811,4.66655,0,1.13952.73254,1.13951a2.5929,2.5929,0,0,0,1.84491-.92246,6.91018,6.91018,0,0,0,1.438-2.279q.05427-.16278.13566-.18992a2.39056,2.39056,0,0,1,.407-.05426h.10853q.407,0,.407.21705a1.332,1.332,0,0,1-.08139.32557,5.10428,5.10428,0,0,1-.48836,1.031,9.64377,9.64377,0,0,1-.92246,1.27517,5.01653,5.01653,0,0,1-1.38368,1.1395,3.32212,3.32212,0,0,1-1.655.46123,3.138,3.138,0,0,1-1.54648-.407,2.35066,2.35066,0,0,1-1.05811-1.41082,3.41144,3.41144,0,0,1-.05426-.73254v-.48836l.94959-3.79836q.94959-3.82548.97672-3.85262a6.66767,6.66767,0,0,0-1.38369-.05426Z"/>
            <path id="MJMAIN-2B-3" data-name="MJMAIN-2B" class="cls-7" d="M464.03769,27.69359v-.35271a.591.591,0,0,1,.37983-.54262h8.11221V22.72859l.02713-4.06966q.27131-.35271.51549-.35271a.59324.59324,0,0,1,.54262.407v8.08507H481.7a.56523.56523,0,0,1,0,1.08524H473.615v8.08508q-.21705.37983-.48836.37983h-.10852a.52593.52593,0,0,1-.48836-.37983V27.8835h-8.11221a.591.591,0,0,1-.37983-.54262Z"/>
            <path id="MJMAIN-31-3" data-name="MJMAIN-31" class="cls-7" d="M489.43241,18.44188l-.3527.13565a7.77512,7.77512,0,0,1-1.08525.27131,12.50484,12.50484,0,0,1-1.5736.18992h-.51549v-1.248h.51549a8.82264,8.82264,0,0,0,2.3604-.407,6.40713,6.40713,0,0,0,1.51935-.65114,6.95615,6.95615,0,0,0,.75967-.59689q.05426-.08139.32557-.08139a.75562.75562,0,0,1,.46123.16279v8.1122l.02713,8.13933a1.07442,1.07442,0,0,0,.32557.24418,3.21738,3.21738,0,0,0,.65115.10853q.51549.05425,1.68213.05426h.70541v1.248H494.94q-.56976-.08139-4.31385-.08139-3.68983,0-4.25959.08139H486.041v-1.248h1.73639a6.51537,6.51537,0,0,0,.67828-.02713,2.65627,2.65627,0,0,0,.4341-.0814,1.62636,1.62636,0,0,1,.217-.05426q.02712,0,.16278-.13565t.16279-.10853Z"/>
          </g>
        </g>
        <circle class="cls-3" cx="87.37025" cy="92.05798" r="17.77298"/>
        <circle class="cls-3" cx="27.97244" cy="151.78489" r="17.77298"/>
        <circle class="cls-3" cx="87.37025" cy="151.86857" r="17.77298"/>
        <circle class="cls-3" cx="147.48061" cy="151.86857" r="17.77298"/>
        <circle class="cls-3" cx="147.48061" cy="210.77372" r="17.77298"/>
        <circle class="cls-3" cx="147.48062" cy="270.58431" r="17.77299"/>
        <circle class="cls-3" cx="467.8298" cy="176.04425" r="39.64484"/>
        <circle class="cls-3" cx="206.15075" cy="91.92074" r="17.77298"/>
        <text class="cls-5" transform="translate(188.49012 37.80558)">+2</text>
        <circle class="cls-3" cx="147.83689" cy="32.59517" r="17.77298"/>
        <circle class="cls-3" cx="147.48061" cy="92.05798" r="17.77298"/>
      </g>
      <g id="downstream">
        <polyline class="cls-8" points="147.331 272.152 147.331 137.682 147.331 88.808"/>
        <path class="cls-8" d="M27.97244,152.13146H69.8445a17.69919,17.69919,0,0,0,17.70937-17.70937l-.04514-24.72538A17.69918,17.69918,0,0,1,105.2181,91.98734l24.48953.07064h17.59692"/>
        <line class="cls-9" x1="467.81917" y1="176.022" x2="366.99624" y2="176.022"/>
        <line class="cls-9" x1="467.8662" y1="280.45165" x2="467.8662" y2="176.09562"/>
      </g>
      <g id="right-path">
        <polygon class="cls-6" points="816.014 171.974 803.189 146.224 803.189 197.724 816.014 171.974"/>
        <line class="cls-10" x1="147.48061" y1="92.05798" x2="245.07162" y2="92.05798"/>
        <g>
          <line class="cls-11" x1="544.36903" y1="176.03214" x2="467.9026" y2="176.03214"/>
          <polygon class="cls-4" points="532.91 160.923 539.329 176.032 532.91 191.141 568.726 176.032 532.91 160.923"/>
        </g>
      </g>
      <g id="up-path">
        <polyline class="cls-12" points="147.481 92.058 147.481 30.965 186.598 30.965"/>
        <polygon class="cls-6" points="770.261 126.265 744.511 139.09 796.01 139.09 770.261 126.265"/>
        <g>
          <line class="cls-13" x1="467.85749" y1="175.93064" x2="467.85749" y2="95.93113"/>
          <polygon class="cls-1" points="482.966 107.39 467.857 100.971 452.749 107.39 467.857 71.575 482.966 107.39"/>
        </g>
      </g>
      <g id="foreground">
        <circle class="cls-14" cx="147.86659" cy="94.16358" r="45.12815"/>
        <circle class="cls-14" cx="469.20775" cy="173.1339" r="124.30276"/>
        <circle class="cls-14" cx="769.20775" cy="173.1339" r="124.30276"/>
        <circle class="cls-14" cx="1069.20775" cy="173.1339" r="124.30276"/>
      </g>
    </svg>


  </div>
  <script src="vis/reweighting.js"></script>

<!-- 
    <div id="test">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 500 500">
      <defs>
        <style>
          .cls-1 {
            fill: #fff;
            stroke: #231f20;
            stroke-miterlimit: 10;
          }
        </style>
      </defs>
      <title>Artboard 1</title>
      <g id="square">
        <rect class="cls-1" x="36.51534" y="45.1227" width="150.30675" height="150.30675"/>
      </g>
      <g id="circle">
        <circle class="cls-1" cx="350.62577" cy="124.2638" r="77.30061"/>
      </g>
    </svg>
  </div> -->
  <!-- <script src="vis/test.js"></script> -->

    <p>
      Wherever intersecting paths are re-weighted, the paths that are most representative of the off-policy distribution end up making larger contributions to the value estimate. Meanwhile, the paths that have lower probability make much smaller contributions.
    </p>


    <h2>Pruning paths with $Q$-learning</h2>

    <p>
      There are many cases where an agent needs to collect experience under a sub-optimal policy while estimating value under an optimal one. For these cases, we use a version of off-policy learning called $Q$-learning.
    </p>

    <style>
      #q-learning {
        width: 700px;
        display: grid;
        grid-template-columns: .4fr 1fr;
        grid-template-rows: auto auto;
        grid-row-gap: 30px;
        grid-column-gap: 30px;
        grid-auto-flow: row;
      }
    </style>
    <figure id="q-learning" class="l-body">
      <div class="figcaption">
        <b>$Q$-learning</b> estimates value under the optimal policy by choosing the max $Q$-value.
      </div>
      <img src="figures/q-learning.svg"/>
    </figure>

   <p>
     $Q$-learning prunes away all but the highest-valued paths. The paths that remain are the paths that the agent will follow at test time; they are the only ones it needs to pay attention to. This sort of value learning often leads to faster convergence than on-policy methods<d-footnote>Try using the Playground at the end of this article to compare between approaches.</d-footnote>.
   </p>

    <p>
      <b>Sometimes $Q$-learning is over-optimistic.</b> There is a problem with $Q$-learning though: it gives <i>biased</i> value estimates. Consider the following example:
    </p>

    <p>
      Imagine you go to a casino and play a hundred slot machines. It's your lucky day: you hit the jackpot on machine 43. Now, if you use $Q$-learning to estimate the value of being in the casino, you will choose the best outcome over the actions of playing slot machines. You'll end up thinking that the value of the casino is the value of the jackpot...and decide that the casino is a great place to be.
    </p>

    <p>
      <b>Double-$Q$ Learning.</b> Sometimes -- as in this casino example -- the largest $Q$-value of a state is large <i>just by chance</i>; choosing it over others makes the value estimate biased.
      One way to reduce this bias is to have a friend visit the casino and play the same set of slot machines. Then, ask them what their winnings were at machine 43 and use their response as your value estimate. It's not likely that you both won the jackpot on the same machine, so this time you won't end up with an over-optimistic estimate. This method of reducing bias is called double $Q$-learning.<d-cite key="VanHasselt2010DoubleQ-learning"></d-cite>
    </p>

    <h2>Reviewing off-policy methods</h2>

    <p>
      It's easy to think of Sarsa, Expected Sarsa, $Q$-learning, and Double $Q$-learning as different algorithms. But as we've seen, they are simply four special cases of off-policy learning.
    </p>

    <style>
      #off-policy-methods {
        width: 700px;
        display: grid;
        grid-template-columns: .4fr 1fr;
        grid-template-rows: auto auto;
        grid-row-gap: 30px;
        grid-column-gap: 30px;
        grid-auto-flow: row;
      }
    </style>
    <figure id="off-policy-methods" class="l-body">
      <div class="figcaption">
        <b>Sarsa</b> uses the $Q$-value associated with $a_{t+1}$ to estimate the next state's value.
      </div>
      <img src="figures/sarsa.svg"/>
      <div class="figcaption">
        <b>Expected Sarsa</b> uses an expectation over $Q$ values to estimate the next state's values.
      </div>
      <img src="figures/expected-sarsa.svg"/>
      <div class="figcaption">
        <b>$Q$-learning</b> estimates value under the optimal policy by choosing the max $Q$-value.
      </div>
      <img src="figures/q-learning.svg"/>
      <div class="figcaption">
        <b>Double $Q$-learning</b> estimates value under the optimal policy by using the action with the max $Q$-value to choose a $Q$-value from a friend's estimate.
      </div>
      <img src="figures/double-q-learning.svg"/>
    </figure>

    <p>
      The intuition behind every one of these approaches is that they re-weight path intersections.
    </p>

    <p>
      <b>Re-weighting paths with Monte Carlo.</b> At this point, a natural question is: Can we accomplish the same re-weighting effect with Monte Carlo? Technically speaking, we could. It would involve weighting each term in the return by a different coefficient:
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

    <figcaption style="grid-row: 1; grid-column: 1/3; max-width:140px;"><b>MC update</b></figcaption>
    <div style="grid-row: 2; grid-column: 1;"><d-math>Q(s_t, a_t)~</d-math></div>
    <div style="grid-row: 2; grid-column: 3;"><d-math>~\hookleftarrow~~</d-math></div>
    <div style="grid-row: 2; grid-column: 5;"><d-math>r_{t}</d-math></div>
    <div style="grid-row: 2; grid-column: 7;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 9;"><d-math>\gamma ~ c_{1} ~ r_{t+1}</d-math></div>
    <div style="grid-row: 2; grid-column: 11;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 13;"><d-math>\gamma^2 ~ c_{2} ~ r_{t+2}</d-math></div>
    <div style="grid-row: 2; grid-column: 15;"><d-math>+~</d-math></div>
    <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>
  </div>

    <p>
      In order to find the re-weighting coefficients $c_1, c_2, ...$, though, we would need to compute statistics across the agent's entire experience. This would end up being far messier than the TD update we saw above.
    </p>

    <p>
      Another way to think about this problem is that, if you want to re-weight paths where they intersect, you need to merge them first. Since Monte Carlo doesn't merge paths, it can't re-weight them without looking back over the full experience.
    </p>


    <h2>Merging Paths with Function Approximators</h2>

    <p>
      Up until now, we've learned one parameter -- the value estimate -- for every state or every state-action pair. This works well for the Cliff World example because it has a small number of states. But as the number of states becomes large or infinite -- as is the case in many interesting RL problems -- it becomes impossible to store a value estimate for each state.
    </p>

  <style>
  #fnapprox-intro {
    margin-left: auto;
    margin-right: auto;
    width: 850px;
    display: grid;
    grid-template-columns: 1fr 1fr 1fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="fnapprox-intro" class="l-page">
      <img src="figures/large-cliffworld-states.svg"/>
      <div class="figcaption">
        <b>Large or infinite state spaces</b> are a characteristic of many interesting RL problems. Value estimation in these spaces often requires function approximation.
      </div>

      <img src="figures/large-cliffworld-path.svg"/>
      <div class="figcaption">
        <b>Tabular value functions</b> keep value estimates for each individual state. They consume a great deal of memory and don't generalize.
      </div>

      <img src="figures/large-cliffworld-approx.svg"/>
      <div class="figcaption">
        <b>Euclidean averagers</b> â€“ which are a type of function approximation â€“ save memory and let agents generalize to states they haven't yet visited.
      </div>
  </figure>

    <p>
      Instead, we must force our value estimator to have fewer parameters than there are states. We can do this with machine learning methods such as linear regression, decision trees, or neural networks. All of these methods fall under the umbrella of function approximation.
    </p>

<!--     <p>
      An interesting property of function approximation is that updating one parameter will tend to change more than one value estimate. Let's see how this changes things by returning to the Cliff World example. This time, we'll give the environment many more states.
    </p> -->

    <p>
      <b>Merging nearby paths.</b> From the paths perspective, we can interpret function approximation as a way of merging nearby paths. But what do we mean by "nearby"? In the figure above, we have made an implicit decision to measure "nearby" with Euclidean distance. This was a good idea, because the Euclidean distance between two states is highly correlated with the probability that the agent will transition between them.
    </p>

    <p>
      However, it's easy to imagine cases where this implicit assumption breaks down. By adding a single long barrier, we can construct a case where the Euclidean distance metric leads to bad generalization. The problem is that we have merged the wrong paths.
    </p>

  <style>
  #fnapprox-barrier {
    margin-left: auto;
    margin-right: auto;
    width: 700px;
    display: grid;
    grid-template-columns: 1fr 1.464fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="fnapprox-barrier" class="l-page">
      <img src="figures/large-cliffworld-barrier-intro.svg"/>
      <div class="figcaption">
        Imagine changing the Cliff World setup by adding a long barrier.
      </div>

      <img src="figures/large-cliffworld-barrier.svg"/>
      <div class="figcaption">
        Now using the Euclidean averager leads to bad value updates.
      </div>
  </figure>

    <p>
      <b>Merging the wrong paths.</b> The diagram below shows the effects of merging the wrong paths a bit more explicitly. Since the Euclidean averager is to blame for poor generalization, both Monte Carlo and TD make bad value updates. However, TD learning amplifies these errors dramatically whereas Monte Carlo does not.
    </p>

  <style>
  #fnapprox-compare {
    margin-left: auto;
    margin-right: auto;
    width: 675px;
    display: grid;
    grid-template-columns: 1fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: column;
  }
  </style>
  <figure id="fnapprox-compare" class="l-page">
      <img src="figures/compare-function-approx.svg"/>
  </figure>
  
    <p>
      We've seen that TD learning makes more efficient value updates. The price we pay, though, is that these updates end up being much more sensitive to bad value updates.
    </p>



    <h2>Implications for deep reinforcement learning</h2>

    <p>
      <b>Neural networks.</b> Deep neural networks are perhaps the most popular function approximators for reinforcement learning. These models are exciting for many reasons, but one particularly nice property is that they don't make implicit assumptions about which states are "nearby."
    </p>

    <p>
      Early in training, neural networks, like averagers, tend to merge the wrong paths of experience. In the Cliff Walking example, an untrained neural network might make the same bad value updates as the Euclidean averager.
    </p>

    <p>
      But as training progresses, neural networks can actually learn to overcome these errors. They learn which states are "nearby" from experience. In the Cliff World example, we might expect a fully-trained neural network to have learned that value updates to states <i>above</i> the barrier should never affect the values of states <i>below</i> the barrier. This isn't something that most other function approximators can do. It's part of the reason why deep RL is so interesting!
    </p>

      <style>
  #nn-latent-distance {
    margin-left: auto;
    margin-right: auto;
    width: 550px;
    display: grid;
    grid-template-columns: 1fr;
    grid-template-rows: auto auto;
    grid-column-gap: 20px;
    grid-row-gap: 10px;
    grid-auto-flow: row;
  }
  </style>
  <figure id="nn-latent-distance" class="l-page">
      <img src="figures/latent-distance.png"/>
      <div class="figcaption">
        A distance metric learned by a neural network <d-cite key="Srinivas2018UniversalNetworks"></d-cite>. <b>Lighter blue $\rightarrow$ more distant</b>. The agent, which was trained to grasp objects using the robotic arm, takes into account obstacles and arm length when it measures the distance between two states.
      </div>
  </figure>

    <p>
      <b>TD or not TD?</b> So far, we've seen how TD learning can outperform Monte Carlo by merging paths of experience where they intersect. We've also seen that merging paths is a double-edged sword: when function approximation causes bad value updates, TD can end up doing worse that Monte Carlo.
    </p>

    <p>
      There's an informal consensus that TD works better than Monte Carlo for deep RL. Indeed, TD methods have dominated the RL literature for the past several decades. There are still some notable exceptions: one recent paper shows that Monte Carlo can still outperform TD methods in certain environments<d-cite key="Amiranashvili2018TDLearning"></d-cite>.
    </p>

    <p>
      Since Monte Carlo and TD learning both have desirable properties, why not try building a value estimator that is a mixture of the two? That's the reasoning behind TD($\lambda$) learning. It's a technique that simply interpolates (using the coefficient $\lambda$) between Monte Carlo and TD updates<d-footnote>In the limit $\lambda=0$, we recover the TD update rule. Meanwhile, when $\lambda=1$, we recover Monte Carlo.</d-footnote>. Often, TD($\lambda$) works better than either Monte Carlo or TD learning alone<d-footnote>Researchers often keep the $\lambda$ coefficient constant as they train a deep RL model. However, if Monte Carlo learning is best early in training (before the agent has learned a good state representation) and TD learning is best later on (when it's easier to benefit from merging paths), maybe the best approach is to anneal $\lambda$ over the course of training!</d-footnote>.
    </p>


    <h2>Conclusion</h2>
    <p>
      An RL agent can improve its policy by looking at separate episodes of experience, one at a time. When it does this, though, it discards a rich source of information: the ways that states from different episodes intersect with one another. The paths perspective is meant to emphasize this second source of information, and to demonstrate how some value estimators, like TD learning, make use of it whereas others, like Monte Carlo, do not.
    </p>

    <p>
      We hope the paths perspective clarifies the relationship between some of the most fundamental approaches to value learning. Understanding these relationships and how they affect the way an agent learns is the first step to making even better RL agents.
    </p>

    <h2>Playground</h2>
    <p>
      We encourage you to verify -- and build upon -- the intuitions outlined in this article using the playground below. Try watching value estimates improve with experience or comparing Monte Carlo to one of the TD value estimators.
    </p>

    <div id="playground"></div>
    <script src="vis/Playground.js" charset="utf-8"></script>




</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    People to thank (living list): Google AI Residency Program, Shan Carter, Ludwig Schubert, Cassandra Xia. Some correspondence with John Schulman and Dale Schuurmans.
  </p>

  <h3 id="variance-analysis-appendix">Variance Analysis</h3>

    <p>
      This section shows a quick derivation of the variance equations presented in the <i>Paths Perspective</i> section. As a broad overview, we will start with the expanded update rules we wrote down in the <i>Beating Monte Carlo</i> section and analyze the variance of each term. Then, using the fact that they are averages over one of more updates, we will analyze the variance of the estimates.
    </p>

    <p>
      Let's start with Monte Carlo. First, we will introduce a new piece of notation, <d-math>T(s)</d-math>, which counts the number of trajectories that pass through state <d-math>s</d-math>.
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

      <figcaption style="grid-row: 1; grid-column: 1/5; max-width:140px;">MC update</figcaption>

      <div style="grid-row: 2; grid-column: 1;"><d-math>U(s_t)~</d-math></div>
      <div style="grid-row: 2; grid-column: 3;"><d-math>=~</d-math></div>
      <div style="grid-row: 2; grid-column: 5;"><d-math>~r_t</d-math></div>
      <div style="grid-row: 2; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 9;"><d-math>\gamma r'_{t+1}</d-math></div>
      <div style="grid-row: 2; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 13;"><d-math>\gamma^2 r''_{t+2}</d-math></div>
      <div style="grid-row: 2; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 4; grid-column: 1/5; max-width:140px;">MC update variance</figcaption>

      <div style="grid-row: 5; grid-column: 1;"><d-math>Var[ U(s_t)]~</d-math></div>
      <div style="grid-row: 5; grid-column: 3;"><d-math>\propto~</d-math></div>
      <div style="grid-row: 5; grid-column: 5;"><d-math>~1</d-math></div>
      <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 9;"><d-math>\gamma</d-math></div>
      <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 13;"><d-math>\gamma^2</d-math></div>
      <div style="grid-row: 5; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 7; grid-column: 1/5; max-width:140px;">MC variance</figcaption>

      <div style="grid-row: 8; grid-column: 1;"><d-math>Var[ V(s_t)]~</d-math></div>
      <div style="grid-row: 8; grid-column: 3;"><d-math>\propto~</d-math></div>
      <div style="grid-row: 8; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 8; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 8; grid-column: 9;"><d-math> \frac{\gamma}{T(s_t)}</d-math></div>
      <div style="grid-row: 8; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 8; grid-column: 13;"><d-math>\frac{\gamma^2 }{T(s_t)}</d-math></div>
      <div style="grid-row: 8; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 8; grid-column: 17;"><d-math>...</d-math></div>

    </div>

    <p>
      Now let's do the same exact thing for TD learning.
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

      <figcaption style="grid-row: 1; grid-column: 1/5; max-width:140px;">TD update</figcaption>

      <div style="grid-row: 2; grid-column: 1;"><d-math>U(s_t)~</d-math></div>
      <div style="grid-row: 2; grid-column: 3;"><d-math>=~</d-math></div>
      <div style="grid-row: 2; grid-column: 5;"><d-math>~r_t</d-math></div>
      <div style="grid-row: 2; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 9;"><d-math>\gamma \mathop{\mathbb{E}} [ r'_{t+1} ]</d-math></div>
      <div style="grid-row: 2; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 13;"><d-math>\gamma^2 \mathop{\mathbb{EE}} [ r''_{t+2} ]</d-math></div>
      <div style="grid-row: 2; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 4; grid-column: 1/5; max-width:140px;">TD update variance</figcaption>

      <div style="grid-row: 5; grid-column: 1;"><d-math>Var[ U(s_t)]~</d-math></div>
      <div style="grid-row: 5; grid-column: 3;"><d-math>\propto~</d-math></div>
      <div style="grid-row: 5; grid-column: 5;"><d-math>~1</d-math></div>
      <div style="grid-row: 5; grid-column: 7;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 9;"><d-math>\frac{\gamma}{ \mathop{\mathbb{E}} [ T(s'_{t+1}) ] }</d-math></div>
      <div style="grid-row: 5; grid-column: 11;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 13;"><d-math>\frac{\gamma^2}{ \mathop{\mathbb{EE}} [T(s''_{t+2})]}</d-math></div>
      <div style="grid-row: 5; grid-column: 15;"><d-math>+</d-math></div>
      <div style="grid-row: 5; grid-column: 17;"><d-math>...</d-math></div>

      <figcaption style="grid-row: 7; grid-column: 9/11; font-size:12px; max-width:140px;">(no change)</figcaption>
      <figcaption style="grid-row: 7; grid-column: 13/15; font-size:12px; max-width:140px;">(no change)</figcaption>

      <figcaption style="grid-row: 8; grid-column: 1/5; max-width:140px;">TD variance</figcaption>

      <div style="grid-row: 9; grid-column: 1;"><d-math>Var[ V(s_t)]~~</d-math></div>
      <div style="grid-row: 9; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 9; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 9; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 9; grid-column: 9;"><d-math> \frac{\gamma}{ \mathop{\mathbb{E}} [ T(s'_{t+1}) ]} </d-math></div>
      <div style="grid-row: 9; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 9; grid-column: 13;"><d-math>\frac{\gamma^2 }{\mathop{\mathbb{EE}} [T(s''_{t+2})]}</d-math></div>
      <div style="grid-row: 9; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 9; grid-column: 17;"><d-math>...</d-math></div>

    </div>


    <p>
      If we rewrite the third line of the Monte Carlo analysis alongside the third line of the TD analysis, we can get the same expression that we wrote down at the end of the <i>Paths Perspective</i> section:
    </p>

    <div class="eq-grid" style="grid-gap: 8px;">

      <figcaption style="grid-row: 1; grid-column: 1; max-width:140px;">MC variance</figcaption>

      <div style="grid-row: 2; grid-column: 1;"><d-math>Var[ V(s_t)]~</d-math></div>
      <div style="grid-row: 2; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 2; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 2; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 9;"><d-math>~\frac{\gamma}{T(s_t)}</d-math></div>
      <div style="grid-row: 2; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 13;"><d-math>~~\frac{\gamma^2}{T(s_t)}</d-math></div>
      <div style="grid-row: 2; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 2; grid-column: 17;"><d-math>...</d-math></div>


      <figcaption style="grid-row: 4; grid-column: 5;"><d-math>~~~~=</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 9;"><d-math>~~~~~~~~\geq</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 13;"><d-math>~~~~~~~~~\geq</d-math></figcaption>
      <figcaption style="grid-row: 4; grid-column: 17;"><d-math>\geq</d-math></figcaption>

      <figcaption style="grid-row: 5; grid-column: 0; max-width:140px;">TD variance</figcaption>

      <div style="grid-row: 6; grid-column: 1;"><d-math>Var[V(s_t)]~~</d-math></div>
      <div style="grid-row: 6; grid-column: 3;"><d-math>~\propto~~</d-math></div>
      <div style="grid-row: 6; grid-column: 5;"><d-math>\frac{1}{T(s_t)}</d-math></div>
      <div style="grid-row: 6; grid-column: 7;"><d-math>~+~</d-math></div>
      <div style="grid-row: 6; grid-column: 9;"><d-math> \frac{\gamma}{ \mathop{\mathbb{E}} [ T(s'_{t+1}) ] }</d-math></div>
      <div style="grid-row: 6; grid-column: 11;"><d-math>~+~</d-math></div>
      <div style="grid-row: 6; grid-column: 13;"><d-math>\frac{\gamma^2 }{\mathop{\mathbb{E}} \mathop{\mathbb{E}} [T(s''_{t+2})]}</d-math></div>
      <div style="grid-row: 6; grid-column: 15;"><d-math>~+~</d-math></div>
      <div style="grid-row: 6; grid-column: 17;"><d-math>...</d-math></div>
    </div>

    <p>
      The next step is to note the 1:1 correspondence of terms in the two sums. For all <d-math>t' > t</d-math>we can say that <d-math>\mathop{\mathbb{E}} [T(s_{t'})] \geq T(s_{t'}) </d-math>. This means that the variance of the TD estimate will be equal or lower than the Monte Carlo estimate.
    </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>

