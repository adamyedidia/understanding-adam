<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style type="text/css">@import "vis/playground.css";</style>
  
  <script src="vis/js/d3.min.js" charset="utf-8"></script>
  <script src="vis/js/underscore.js" charset="utf-8"></script>
  <script src="vis/js/rounding.js" charset="utf-8"></script>
  <script src="vis/util.js" charset="utf-8"></script>
  <script src="vis/GridWorld.js" charset="utf-8"></script>
  <script src="vis/LearnQV.js" charset="utf-8"></script>
  <script src="vis/Policy.js" charset="utf-8"></script>
  <script src="vis/Aprox.js" charset="utf-8"></script>

  <script src="vis/cliff.js" charset="utf-8"></script>
  <script src="vis/compare.js" charset="utf-8"></script>
  <script src="vis/tug.js" charset="utf-8"></script>
  <script src="vis/tug_baseline.js" charset="utf-8"></script>
  <script src="vis/reinforce.js" charset="utf-8"></script>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "An Intersecting Paths Perspective on RL",
  "description": "",
  "password": "td",
  "authors": [

    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }

    {
      "author": "Sam Greydanus",
      "authorURL": "https://greydanus.github.io/about.html",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "strict": false,
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<style>
  #cliff_iter .cell rect {
    fill: #e7eae7;
  }
  .cell rect {
    fill: #e7eae7;
  }
</style>

<d-title>
  <h1>An Intersecting Paths Perspective on RL</h1>
  <p>Temporal Difference (TD) learning merges paths for greater statistical efficiency. What are the consequences?</p>

  <figure class="l-page">
    <img style="width: 100%;" src="mockups/hero.png"/>
  </figure>
</d-title>

<!--   <figure class="l-page">
    <div id="compare_hero" style="width:1000px; height:350px; margin:auto; position:relative;"> </div>
  </figure>
  <script> compare_vis(d3.select("#compare_hero"), {algs: ["MC", "TD", "Q"] }); </script> 
</d-title> -->

<d-article>

  <p>
    After making an important decision, we often ask ourselves: “What would have happened if I had acted differently?” One of the most beautiful and daunting aspects of life is that we can never know. The best we can do is consider previous experience and make a guess…
  </p>

  <p>
    Reinforcement learning agents face the same challenge. At each decision point, they can choose only one action. When they observe the rewards for an action, they have no way of knowing whether a different action would have produced higher rewards. They, too, must learn from experience.
  </p>

  <p>
    Monte Carlo Learning and Temporal Difference (TD) Learning are two methods that agents use to do this. Sometimes we think of TD as a practical approximation of Monte Carlo, but there is more to the story. TD can outperform Monte Carlo by a large margin, something that we wouldn’t expect of an approximation.
  </p>

  <p>
    In this post, we introduce a different intuition. We encourage the reader to think of experience as a series of overlapping paths. Then we show that TD methods merge these paths for greater statistical efficiency whereas MC methods keep them separate. When TD merges the right paths, it makes huge gains over Monte Carlo. When it merges the wrong paths, it ends up doing worse.
  </p>


    <h3>The Reinforcement Learning Paradigm</h3>

    <p>
      Our story begins with an agent and an environment. The agent gets to observe, interact with, and receive rewards from the environment. As it explores and gains experience, it tries to make sense of what actions led to the highest long-term reward.
    </p>

    <figure class="l-page" style="width: 100%; margin-top: 15px;">
      <center>

        <img style="width: 80%; margin-top: 15px; " src="mockups/concepts.png"/>
      <figcaption class="l-body" style="width: 100%;>
        <a href="#figure-rgba-diagram" class="figure-number">1</a>: Key Concepts
          <div class="eq-grid" style="grid-gap: 1%">
            <div style="grid-row: 1; grid-column: 1;">
              <b>Policy.</b> When we start off with a completely random policy, the agent tends to fall off the cliff often. Thus the value estimate under this policy assigns low value to any state near the cliff.
            </div>
            <div style="grid-row: 1; grid-column: 2;">
              <b>Experience.</b> Having assigned low value to states near the cliff, the agent updates its policy in order to avoid those states. <!-- We may allow it to take an occasional random action, in which case it could still fall off the cliff, but this will happen only rarely. --> With a policy that avoids the cliff, the agent learns to assign higher value to actions that take it upwards and rightwards towards its destination.
            </div>
            <div style="grid-row: 1; grid-column: 3;">
              <b>State-value function.</b> Almost there! The agent has not quite converged to an optimal policy, though, because the bottom row still has left-facing transitions. The value update #2 reveals that these transitions are sending the agent to states of slightly lower value.
          </div>
            <div style="grid-row: 1; grid-column: 4;">
              <b>Action-value function.</b> An optimal policy. It moves only upward and rightward and avoids transitions that take it over the cliff.
            </div>
          </div>
      </figcaption>

      </center>
    </figure>

    <p>
      <b>Measuring long-term reward.</b> One of the nuances of this setup is measuring long-term reward. We often use a quantity called the time-discounted sum of rewards, or simply <i>the return</i>. Since the return is a sum over future rewards, it reinforces actions that lead not only to rewards at the next time step, but also to rewards over the course of hundreds or even thousands of time steps. This encourages the agent to think long term even without an explicit planning mechanism.
    </p>

    <center><p>$$R = \sum_t \gamma^t r_t$$ </p> </center>

    <p>
      The other thing to notice about the return is that rewards received $$k$$ time steps in the future are $$\gamma^{k-1}$$ times what they would be worth if the agent received them immediately. By setting the discount factor $$\gamma$$ to values less than one, we bias the agent towards earning more immediate rewards.
    </p>

    <p>
      <b>From reward to value.</b> Most of the time, we are not interested in how much return an agent received after passing through a particular state; we'd rather know how much the agent can expect to receive, in general, after passing through that state. We call this quantity the value of the state and measure it using a <i>state-value</i> function $$V(s)$$.
    </p>

    <p>
<!--       <b>Why value matters.</b> We can measure the value of other contexts, apart from states. Some contexts we'll touch on in this article include perturbed policies, partial states, and state-action pairs ($$Q$$-values). The second nice property of value is that Whenever an agent does something to increase its value, it increases its expected long-term reward. In other words, an agent can arrive at a strong long-term policy just by following a local value gradient. -->

      The remarkable thing about value is that an agent can maximize its long-term reward just by following local gradients of $$V(s)$$. In other words, the optimal policy of an agent is to choose actions which maximize $$V(s)$$ at every step. Any long-term planning that the agent might need to do gets integrated into our value function when we sum over discounted rewards.
    </p>

    <h3>Aside: Function approximation</h3>

    <p>
      Function approximators are important to this post and appear often enough that they deserve their own notation. We will write them as functions with hats and update them according to $$\hat f(x)_{k} ~\hookleftarrow~ y$$. This notation makes things like running averages look much more benign.
    </p>

    <div class="eq-grid">
      <div style="grid-row: 1; grid-column: 1;"><d-math> V_k ~~ </d-math></div>
      <div style="grid-row: 1; grid-column: 2;"><d-math> =~~ \frac{1}{k} \sum_{i=0}^{k} R_i </d-math></div>
      <div style="grid-row: 2; grid-column: 2;"><d-math> =~~ V_{k-1} + \alpha [R_{k-1} - V_{k-1}] ~~~~</d-math></div>
      <div style="grid-row: 3; grid-column: 1;"><d-math> V_k ~~ </d-math></div>
      <div style="grid-row: 3; grid-column: 2;"><d-math> \hookleftarrow~~ R_k </d-math></div>
     
      <figcaption style="grid-row: 1; grid-column: 3; max-width:300px;">
        Averages are usually expressed as a sum.
      </figcaption>

      <figcaption style="grid-row: 2; grid-column: 3; max-width:300px;">
        Sometimes it makes more sense to keep a running average.
      </figcaption>

      <figcaption style="grid-row: 3; grid-column: 3; max-width:300px;">
        Our notation for a running average update.
      </figcaption>
    </div>

    <p>
      One note on running averages is that we need to set $$\alpha=\frac{1}{k}$$ if we want to compute an <i>exact</i> running average. In practice, we can usually obtain the same empirical results by letting $$\alpha$$ be a small constant.
    </p>

    <h2>Policy Value Iteration</h2>
    <br>

    <p>
      In this section we'll see how an agent can use value estimates to improve its policy. Let's start of with a simple example in which the agent needs to learn how to walk along a cliff without falling off.
    </p>

    <p>
      <b>Example: Cliff Walking.</b> Welcome to the cliff walking environment! Our goal is to reach the destination (blue square, +1 reward) and avoid falling off of the cliff (red rectangle, -1 reward). The agent's initial policy is to move into any of the neighboring states with equal probability.
    </p>

    <figure class="l-page">
    <div id="cliff_iter" style="width:600px; height:500px; position:relative;"> </div>
    
    <center>
      <figcaption class="l-body" style="width:100%;" >
        <a href="#figure-rgba-diagram" class="figure-number">2</a>: Cliff Walking
        <div class="eq-grid" style="grid-gap: 1%">
          <div style="grid-row: 1; grid-column: 1;">
            <b>Step 0.</b> When we start off with a completely random policy, the agent tends to fall off the cliff often. Thus the value estimate under this policy assigns low value to any state near the cliff.
          </div>
          <div style="grid-row: 1; grid-column: 2;">
            <b>Step 1.</b> Having assigned low value to states near the cliff, the agent updates its policy in order to avoid those states. <!-- We may allow it to take an occasional random action, in which case it could still fall off the cliff, but this will happen only rarely. --> With a policy that avoids the cliff, the agent learns to assign higher value to actions that take it upwards and rightwards towards its destination.
          </div>
          <div style="grid-row: 1; grid-column: 3;">
            <b>Step 2.</b> Almost there! The agent has not quite converged to an optimal policy, though, because the bottom row still has left-facing transitions. The value update #2 reveals that these transitions are sending the agent to states of slightly lower value.
        </div>
          <div style="grid-row: 1; grid-column: 4;">
            <b>Step 3.</b> An optimal policy. It moves only upward and rightward and avoids transitions that take it over the cliff.
          </div>
        </div>
      </div>
      </figcaption>
    </center>
    </figure>
    <script> cliff_vis(d3.select("#cliff_iter")); </script>

    <p>
      <b>Policy Value Iteration.</b> The cycle that takes the agent from policy, to experience, to value estimate, and back again is called policy value iteration
      <d-footnote>
        People usually lump experience into the policy step, hence "policy value iteration" instead of "policy experience value iteration." But we chose not to do this because the "paths" interpretation of experience is important to our thesis.
      </d-footnote>.
      As the cycle runs, the agent's policy improves, its experience increases, and its value estimates become more precise. The agent "grows up."
    </p>

    <figure id="figure-policy-value-iteration" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/policy-value-iteration.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">3</a>:
        Policy value iteration.
        <br>
         <b>Experience.</b> For a simple environment like cliff walking, we can compute value estimates analytically. In more complicated environments, though, this is impossible. Instead, the agent must run a its policy stochastically, gain experience, and then use its experience to make value estimates.
      </figcaption>
    </figure>

    <p>
      You might imagine how a small difference in the way an agent estimates value might have big consequences for its policy update, future experience, and the way it updates its value estimates the next time.
    </p>


    <h2>Merging paths to beat Monte Carlo</h2>
    <br>
    
    <figure id="figure-mc-td" class="subgrid" style="margin-top: 0;">
      <center>
        <img style="width: 80%; margin-top: 15px;" src="mockups/mc-td.png"/>
      </center>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">6</a>:
        Monte Carlo Learning vs. Temporal Difference Learning.
      </figcaption>
    </figure>

    <br>
    <p>$$V(s_t) ~\hookleftarrow~ r_t + \gamma V(s_{t+1})$$</p>
  


    <figure id="figure-mc-td-animation" class="subgrid" style="margin-top: 0;">
      <center>
        <img style="width: 80%; margin-top: 15px;" src="mockups/mc-td-animation.png"/>
      </center>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">6</a>:
        Animation.
      </figcaption>
    </figure>

<!--     <figure class="l-page">
    <div id="compare2" style="width:650px; height:500px; margin:auto; position:relative;"> </div>
    </figure>
    <script> compare_vis(d3.select("#compare2"), {algs: ["MC", "TD"] }); </script> -->





    <h2>Re-weighting the future: Q-values</h2>
    <br>
    <p>State-Action Values</p>
    <p>Easier to make policies</p>
    <p>MC and TD still work.</p>
    <p>“Q functions are split value state value functions.” How do we merge them back </p>

    <p>Why Q-learning is overconfident</p>
    <br>






    <h2>TD Learning and State Approximation</h2>
    <br>
    <p>Neural networks, so forth</p>

    <img style="width: 100%; margin-top: 15px" src="mockups/averagers.png"/>






    <h2>Playground</h2>
    <br>
    <div id="playground"></div>
    <script src="vis/Playground.js" charset="utf-8"></script>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
