<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style type="text/css">@import "vis/playground.css";</style>
  
  <script src="vis/js/d3.min.js" charset="utf-8"></script>
  <script src="vis/js/underscore.js" charset="utf-8"></script>
  <script src="vis/js/rounding.js" charset="utf-8"></script>
  <script src="vis/util.js" charset="utf-8"></script>
  <script src="vis/GridWorld.js" charset="utf-8"></script>
  <script src="vis/LearnQV.js" charset="utf-8"></script>
  <script src="vis/Policy.js" charset="utf-8"></script>
  <script src="vis/Aprox.js" charset="utf-8"></script>

  <script src="vis/cliff.js" charset="utf-8"></script>
  <script src="vis/compare.js" charset="utf-8"></script>
  <script src="vis/tug.js" charset="utf-8"></script>
  <script src="vis/tug_baseline.js" charset="utf-8"></script>
  <script src="vis/reinforce.js" charset="utf-8"></script>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "An Intersecting Paths Perspective on RL",
  "description": "",
  "password": "td",
  "authors": [

    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }

    {
      "author": "Sam Greydanus",
      "authorURL": "https://greydanus.github.io/about.html",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "strict": false,
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<style>
  #cliff_iter .cell rect {
    fill: #e7eae7;
  }
  .cell rect {
    fill: #e7eae7;
  }
</style>

<d-title>
  <h1>An Intersecting Paths Perspective on RL</h1>
  <p>Temporal Difference (TD) learning merges paths for greater statistical efficiency. What are the consequences?</p>
  <figure class="l-page">
    <div id="compare_hero" style="width:1000px; height:350px; margin:auto; position:relative;"> </div>
  </figure>
  <script> compare_vis(d3.select("#compare_hero"), {algs: ["MC", "TD", "Q"] }); </script> 
</d-title>

<d-article>

  <p>
    After making an important decision, we often ask ourselves: “What would have happened if I had acted differently?” One of the most beautiful and daunting aspects of life is that we can never know. The best we can do is consider previous experience and make a guess…
  </p>

  <p>
    Reinforcement learning agents face the same challenge. At each decision point, they can choose only one action. When they observe the rewards for an action, they have no way of knowing whether a different action would have produced higher rewards. They, too, must learn from experience.
  </p>

  <p>
    Monte Carlo Learning and Temporal Difference (TD) Learning are two methods that agents use to do this. Sometimes we think of TD as a practical approximation of Monte Carlo, but there is more to the story. TD can outperform Monte Carlo by a large margin, something that we wouldn’t expect of an approximation.
  </p>

    <center> <p> Figure: TD outperforming MC? </p> </center>

  <p>
    In this post, we introduce a different intuition. We encourage the reader to think of experience as a series of overlapping paths. Then we show that TD methods merge these paths for greater statistical efficiency whereas MC methods keep them separate. When TD merges the right paths, it makes huge gains over Monte Carlo. When it merges the wrong paths, it ends up doing worse.
  </p>


    <h3>The Reinforcement Learning Paradigm</h3>

    <p>
      Our story begins with an agent and an environment. The agent gets to observe, interact with, and receive rewards from the environment. As it explores and gains experience, it tries to make sense of what actions led to the highest long-term reward.
    </p>

    <figure id="figure-concepts" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/concepts.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">1</a>:
        Introduce policy, experience, and value functions in the captions beneath these figures. Also touch on the concept of a state.
      </figcaption>
    </figure>

    <p>
      The idea that we are maximizing long-term rewards is important. We’d like our agent to balance short-term rewards against long-term ones. A In the classic psychology experiment which offers, “one marshmallow now or two later” we’d like an agent that chooses the second option. But receiving one marshmallow now is probably better than receiving one marshmallow later. We would also like our agent to “discount” rewards in proportion to how much time it takes to earn them. The quantity that combines these two ideas is the “sum of time-discounted rewards,” or return, of an agent
    </p>

    <center><p>$$R = \sum_t \gamma^t r_t$$ </p> </center>

    <p>
      Depending on context, an agent can expect to receive a certain amount of return. We call this the value of the context. When the context is simply the agent’s state, we measure value with a state-value function. Sometimes an agent has also selected an action; we measure the value of this context with an action-value function.
    </p>


    <h2>Policy Value Iteration</h2>
    <br>

        <h3>Example: Cliff walking</h3>
    <br>

    <figure id="figure-cliff-walking" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/cliff-walking.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">3</a>:
        Cliff walking.
      </figcaption>
    </figure>
<!--     <figure class="l-page">
    <div id="cliff_iter" style="width:600px; height:500px; position:relative;"> </div>
    </figure>
    <script> cliff_vis(d3.select("#cliff_iter")); </script> -->

    <p>
      You may have noticed that there is a natural cycle from an agent's policy to experience to value estimate, and back again. As the agent follows this cycle, its policy improves, its experience increases, and its value estimates become more precise. The agent "grows up." This is a process called <i>policy value iteration</i>
        <d-footnote>
          Sometimes people lump experience into the policy step, hence "policy value iteration" rather than "policy experience value iteration." But we chose not to do this because the "paths" interpretation of experience is important to our thesis.
        </d-footnote>
    </p>

    <figure id="figure-policy-value-iteration" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/policy-value-iteration.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">2</a>:
        Policy value iteration.
      </figcaption>
    </figure>

    <p>
      You might imagine how a small difference in the way an agent estimates value might have big consequences for its policy update, future experience, and the way it updates its value estimates the next time.
    </p>

    <!-- <p>
      <b>Running the policy.</b> This is how the agent updates its experience. Some algorithms do this for only one time step, whereas others will run an entire episode or batch of episodes. Some algorithms keep a replay memory of all the agent's training experience whereas others do not store experience at all.
    </p>
    <p>
      <b>Value learning.</b> In this step, the agent updates its value estimate.
    </p>
      <center><p>$$R = \sum_t \gamma^t r_t$$ </p> </center>
    <p>
      The most obvious way to estimate value from returns is just to take the average:.
      <center><p>$$\hat V(s_i) = \frac{1}{N} \sum_{s'=s} R_{s'}$$ </p> </center>
      This is just Monte Carlo value estimation. The surprising fact is that we can do better than this.
    </p>
    <p>
      <b>Policy update.</b> Once the agent has refined its value estimate, ...
    </p> -->


    <h2>Aside: Function approximation</h2>
    <ul>
      <li>Evaluate: $$~~~f(x)$$</li>
      <li>Update: $$~~~~~f(x) ~\hookleftarrow~ y$$</li>
    </ul>



    <h2>Merging paths to beat Monte Carlo</h2>
    <br>
    
    <figure id="figure-mc-vs-td" class="subgrid" style="margin-top: 0;">
      <center>
        <img style="width: 80%; margin-top: 15px;" src="mockups/mc-vs-td.png"/>
      </center>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">4</a>:
        Monte Carlo Learning vs. Temporal Difference Learning.
      </figcaption>
    </figure>

    <br>
    <p>$$V(s_t) ~\hookleftarrow~ r_t + \gamma V(s_{t+1})$$</p>
  

    
    <figure class="l-page">
    <div id="compare2" style="width:650px; height:500px; margin:auto; position:relative;"> </div>
    </figure>
    <script> compare_vis(d3.select("#compare2"), {algs: ["MC", "TD"] }); </script>





    <h2>Re-weighting the future: Q-values</h2>
    <br>
    <p>State-Action Values</p>
    <p>Easier to make policies</p>
    <p>MC and TD still work.</p>
    <p>“Q functions are split value state value functions.” How do we merge them back </p>

    <p>Why Q-learning is overconfident</p>
    <br>






    <h2>TD Learning and State Approximation</h2>
    <br>
    <p>Neural networks, so forth</p>

    <img style="width: 100%; margin-top: 15px" src="mockups/averagers.png"/>






    <h2>Playground</h2>
    <br>
    <div id="playground"></div>
    <script src="vis/Playground.js" charset="utf-8"></script>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
