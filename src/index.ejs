<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style type="text/css">@import "vis/playground.css";</style>
  
  <script src="vis/js/d3.min.js" charset="utf-8"></script>
  <script src="vis/js/underscore.js" charset="utf-8"></script>
  <script src="vis/js/rounding.js" charset="utf-8"></script>
  <script src="vis/util.js" charset="utf-8"></script>
  <script src="vis/GridWorld.js" charset="utf-8"></script>
  <script src="vis/LearnQV.js" charset="utf-8"></script>
  <script src="vis/Policy.js" charset="utf-8"></script>
  <script src="vis/Aprox.js" charset="utf-8"></script>

  <script src="vis/cliff.js" charset="utf-8"></script>
  <script src="vis/compare.js" charset="utf-8"></script>
  <script src="vis/tug.js" charset="utf-8"></script>
  <script src="vis/tug_baseline.js" charset="utf-8"></script>
  <script src="vis/reinforce.js" charset="utf-8"></script>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "An Intersecting Paths Perspective on RL",
  "description": "",
  "password": "td",
  "authors": [

    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }

    {
      "author": "Sam Greydanus",
      "authorURL": "https://greydanus.github.io/about.html",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "strict": false,
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<style>
  #cliff_iter .cell rect {
    fill: #e7eae7;
  }
  .cell rect {
    fill: #e7eae7;
  }
</style>

<d-title>
  <h1>An Intersecting Paths Perspective on RL</h1>
  <p>Temporal Difference (TD) learning merges paths for greater statistical efficiency. What are the consequences?</p>
  <figure class="l-page">
    <div id="compare_hero" style="width:1000px; height:350px; margin:auto; position:relative;"> </div>
  </figure>
  <script> compare_vis(d3.select("#compare_hero"), {algs: ["MC", "TD", "Q"] }); </script> 
</d-title>

<d-article>

  <p>
    After making an important decision, we often ask ourselves: “What would have happened if I had acted differently?” One of the most beautiful and daunting aspects of life is that we can never know. The best we can do is consider previous experience and make a guess…
  </p>

  <p>
    Reinforcement learning agents face the same challenge. At each decision point, they can choose only one action. When they observe the rewards for an action, they have no way of knowing whether a different action would have produced higher rewards. They, too, must learn from experience.
  </p>

  <p>
    Monte Carlo Learning and Temporal Difference (TD) Learning are two methods that agents use to do this. Sometimes we think of TD as a practical approximation of Monte Carlo, but there is more to the story. TD can outperform Monte Carlo by a large margin, something that we wouldn’t expect of an approximation.
  </p>

  <p>
    In this post, we introduce a different intuition. We encourage the reader to think of experience as a series of overlapping paths. Then we show that TD methods merge these paths for greater statistical efficiency whereas MC methods keep them separate. When TD merges the right paths, it makes huge gains over Monte Carlo. When it merges the wrong paths, it ends up doing worse.
  </p>


    <h3>The Reinforcement Learning Paradigm</h3>

    <p>
      Our story begins with an agent and an environment. The agent gets to observe, interact with, and receive rewards from the environment. As it explores and gains experience, it tries to make sense of what actions led to the highest long-term reward.
    </p>

    <figure id="figure-concepts" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/concepts.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">1</a>:
        Introduce policy, experience, and value functions in the captions beneath these figures. Also touch on the concept of a state.
      </figcaption>
    </figure>

    <p>
      <b>Measuring long-term reward.</b> One of the nuances of this setup is measuring long-term reward. If the agent forgoes some short term rewards in order to "set itself up" for large rewards later on, we should encourage that decision. But we'd like to maintain a slight bias towards short-term rewards: a unit of reward now should be worth more than a unit of reward far in the future. A quantity which captures both of these ideas is the time-discounted sum or rewards, or return:
    </p>

    <center><p>$$R = \sum_t \gamma^t r_t$$ </p> </center>

    <p>
      Now suppose we want to know how much long term reward an agent can <i>expect</i> to receive for being in a particular state. We can simply take the expectation value of all the returns the agent has received when in that state. We call this the value $$V(s)$$ of state $$s$$.
    </p>

    <p>
      <b>Two nice properties of value.</b> Value ends up being an immensely useful quantity. First, we can measure the value of other contexts, apart from states. Some contexts we'll touch on in this article include state-action pairs (denoted $$Q(s,a)$$), perturbed policies, and partial states. The second nice property of value is that whenever an agent does something to increase its value, it effectively increases its expected long-term reward. In other words, an agent can arrive at a strong long-term policy just by following a local value gradient.
    </p>

    <h2>Policy Value Iteration</h2>
    <br>

    <p>
      In this section we'll see how an agent can use value estimates to improve its policy. Let's start of with a simple example in which the agent needs to learn how to walk along a cliff without falling off.
    </p>

    <p>
      <b>Example: Cliff Walking.</b> Welcome to the cliff walking environment! Our goal is to reach the destination (blue square, +1 reward) and avoid falling off of the cliff (red rectangle, -1 reward). The agent's initial policy is to move into any of the neighboring states with equal probability.
    </p>

    <figure class="l-page">
    <div id="cliff_iter" style="width:600px; height:500px; position:relative;"> </div>
    
    <center>
      <figcaption class="l-body" style="width:400px;" >
        <a href="#figure-rgba-diagram" class="figure-number">2</a>:
        <b>Cliff walking:</b>
          <li>Step 0: When we start off with a completely random policy, the agent tends to fall off the cliff often. Thus the value estimate under this policy assigns low value to any state near the cliff.</li>
          <li>Step 1: Having assigned low value to states near the cliff, the agent updates its policy in order to avoid those states. <!-- We may allow it to take an occasional random action, in which case it could still fall off the cliff, but this will happen only rarely. --> With a policy that avoids the cliff, the agent learns to assign higher value to actions that take it upwards and rightwards towards its destination. </li>
          <li>Step 2: Almost there! The agent has not quite converged to an optimal policy, though, because the bottom row still has left-facing transitions. The value update #2 reveals that these transitions are sending the agent to states of slightly lower value. </li>
          <li>Step 3: An optimal policy. It moves only upward and rightward and avoids transitions that take it over the cliff.</li>
      </figcaption>
    </center>
    </figure>
    <script> cliff_vis(d3.select("#cliff_iter")); </script>

    <p>
      <b>Experience.</b> For a simple environment like cliff walking, we can compute value estimates analytically. In more complicated environments, though, this is impossible. Instead, the agent must run a its policy stochastically, gain experience, and then use its experience to make value estimates.
    </p>

    <figure id="figure-policy-value-iteration" class="subgrid" style="margin-top: 0;">
      <img style="width: 100%; margin-top: 15px;" src="mockups/policy-value-iteration.png"/>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">3</a>:
        Policy value iteration.
      </figcaption>
    </figure>

    <p>
      <b>Policy Value Iteration.</b> The cycle that takes the agent from policy, to experience, to value estimate, and back again is called policy value iteration
      <d-footnote>
        People usually lump experience into the policy step, hence "policy value iteration" instead of "policy experience value iteration." But we chose not to do this because the "paths" interpretation of experience is important to our thesis.
      </d-footnote>.
      As the cycle runs, the agent's policy improves, its experience increases, and its value estimates become more precise. The agent "grows up."
    </p>

    <p>
      You might imagine how a small difference in the way an agent estimates value might have big consequences for its policy update, future experience, and the way it updates its value estimates the next time.
    </p>


    <h2>Aside: Function approximation</h2>
    <ul>
      <li>Evaluate: $$~~~f(x)$$</li>
      <li>Update: $$~~~~~f(x) ~\hookleftarrow~ y$$</li>
    </ul>



    <h2>Merging paths to beat Monte Carlo</h2>
    <br>
    
    <figure id="figure-mc-vs-td" class="subgrid" style="margin-top: 0;">
      <center>
        <img style="width: 80%; margin-top: 15px;" src="mockups/mc-vs-td.png"/>
      </center>
      <figcaption class="l-body">
        <a href="#figure-rgba-diagram" class="figure-number">6</a>:
        Monte Carlo Learning vs. Temporal Difference Learning.
      </figcaption>
    </figure>

    <br>
    <p>$$V(s_t) ~\hookleftarrow~ r_t + \gamma V(s_{t+1})$$</p>
  

    
    <figure class="l-page">
    <div id="compare2" style="width:650px; height:500px; margin:auto; position:relative;"> </div>
    </figure>
    <script> compare_vis(d3.select("#compare2"), {algs: ["MC", "TD"] }); </script>





    <h2>Re-weighting the future: Q-values</h2>
    <br>
    <p>State-Action Values</p>
    <p>Easier to make policies</p>
    <p>MC and TD still work.</p>
    <p>“Q functions are split value state value functions.” How do we merge them back </p>

    <p>Why Q-learning is overconfident</p>
    <br>






    <h2>TD Learning and State Approximation</h2>
    <br>
    <p>Neural networks, so forth</p>

    <img style="width: 100%; margin-top: 15px" src="mockups/averagers.png"/>






    <h2>Playground</h2>
    <br>
    <div id="playground"></div>
    <script src="vis/Playground.js" charset="utf-8"></script>

</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
