<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
  <style type="text/css">@import "vis/playground.css";</style>

  <script src="vis/js/d3.min.js" charset="utf-8"></script>
  <script src="vis/js/underscore.js" charset="utf-8"></script>
  <script src="vis/js/rounding.js" charset="utf-8"></script>
  <script src="vis/util.js" charset="utf-8"></script>
  <script src="vis/GridWorld.js" charset="utf-8"></script>
  <script src="vis/LearnQV.js" charset="utf-8"></script>

  <script src="vis/compare.js" charset="utf-8"></script>

</head>


<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Understanding Adam",
  "description": "A closer look at the dynamics of Adam, pre- and post-convergence",
  "authors": [

    {
      "author": "Adam Yedidia",
      "authorURL": "adamyedidia.wordpress.com",
      "affiliation": "MIT",
      "affiliationURL": "mit.edu"
    },

    {
      "author": "Sadhika Malladi",
    }
    {
      "author": "Chris Hesse",
    }
    {
      "author": "John Schulman",
    }
    {
      "author": "Jacob Hilton",
    }
  ],
  "katex": {
    "strict": false,
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Understanding Adam</h1>
  <p>
   A closer look at the dynamics of Adam, pre- and post-convergence
  </p>

</d-title>

<d-article>
<!-- <d-cite key="Tsividis2017HumanAtari"></d-cite> -->

  <h2>Introduction</h2>
  <p>
# Adam is Unstable on Quadratic Loss Functions

This memo is a brief analysis of Adam, and how it can behave unstably. This memo is mainly meant as an addendum to the memo *Mitigating Loss Spikes*; it's self-contained, but it's quite a bit denser and as such you might have some difficulty understanding it if you're not already well-familiarized with Adam or if you didn't come here from *Mitigating Loss Spikes*. For a gentler explanation of Adam, see the section on how Adam works in *Mitigating Loss Spikes*, or see this blog post on different optimization algorithms. If you want, you can skip straight ahead to the [TL;DR section](#tldr), which will tell you how to detect if this issue is bottlenecking you and what to do about it if it is.

## Motivation

The 1D quadratic loss function is arguably the simplest possible convex loss function. Moreover, because all smooth real-world loss functions are locally quadratic, and because Adam acts on each of its dimensions independently, we can expect at least some of the our conclusions about Adam on the 1D quadratic to carry over to real-world functions, at least after we've partly converged to a local optimum (since that optimum is likely to be locally quadratic). An analysis on the same model (though with a different focus) can be found in this paper by Zhang et al. Additionally, it's previously been demonstrated that Adam doesn't converge on all convex functions, as initially claimed; the counterexample is presented in this paper by Reddi et al. I'll go further and show that Adam doesn't converge on all quadratic functions.

## Model

Our loss function will be $y_t = cx_t^2$, for some constant $c$. At each iteration, we'll observe a noisy gradient:

$$g_t = \frac{dy}{dx}(t) + \xi = 2cx_t + \xi$$

Here $\xi$ is an additive noise term which we'll take to be normally distributed with mean 0 and variance $\sigma_{\xi}^2$. Often when I analyze the model in its simplest possible case I'll assume $\sigma_\xi = 0$.

## Adam Update Rule

Adam's update rule is as follows:

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$
$$\hat{m}_t = m_t/(1 - \beta_1^t)$$
$$\hat{v}_t = v_t/(1 - \beta_2^t)$$
$$x_{t+1} = x_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon}\hat{m}_t$$

Adam's hyperparameters here are $\beta_1$ and $\beta_2$, which control the length of the exponential averages of the first and second moments respectively, the learning rate $\eta$, and $\epsilon$.

The third and fourth equations above constitute Kingma and Ba's "bias correction" for at the start of optimization; they make a negligible difference in the limit as $t \rightarrow \infty$ (more precisely, when $\beta_1^t \ll 1$ and $\beta_2^t \ll 1$), and as such, I will sometimes neglect them and assume $\hat{m}_t = m_t$ and $\hat{v}_t = v_t$, since the focus of this memo is on Adam's steady-state behavior.

## Fixed points of Adam 

When running Adam on the 1D quadratic function with a given set of hyperparameters and no noise, there are many different possible different repeating sequences of observations $x$. The simplest ones of these I'll call "fixed points": basic oscillating sequences where $x_{t+1} = -x_t$ for all $t$ (including $t<0$; for simplicity, these sequences extend infinitely in both directions). These fixed points aren't always stable, and reaching them isn't the typical case when Adam is run on a noiseless quadratic, but they are easy to calculate and I believe that Adam's nontrivial fixed points often reveal something about how Adam will behave for that set of hyperparameters. 

For every set of hyperparameters with $\epsilon > 0$, there is a trivial fixed point: $x_t = 0$ for all $t$. This is the set of fixed points caused by starting the search at the optimum. This is the only situation in which Adam reaches the function's true optimum and stays there even in the noiseless case, and even in the limit as $t \rightarrow \infty$. Moreover, this set of fixed points only exists in the first place because of the hyperparameter $\epsilon$.

Let us suppose that we are at the fixed point when $x_t = x^*$ for all even $t$ and $x_t = -x^*$ for all odd $t$. We can derive the nontrivial fixed point by unwrapping the infinite sums $v_T$ and $m_T$ (because this is in steady-state, we can assume $\hat{v}_T = v_T$ and $\hat{m}_T = m_T$):

$$m_T = (1-\beta_1)\sum_{t = -\infty}^T \beta_1^{T-t} g_t= (1-\beta_1)\sum_{t = -\infty}^T \beta_1^{T-t} 2cx_t$$
$$m_T = (1-\beta_1)\sum_{t = -\infty}^T (-\beta_1)^{T-t} 2cx^* = 2cx^* (1-\beta_1) \sum_{t = -\infty}^T (-\beta_1)^{T-t}$$
$$m_T = 2cx^*\frac{1-\beta_1}{1+\beta_1}$$

$$v_T = (1-\beta_2)\sum_{t = -\infty}^T \beta_2^{T-t} g_t^2 = (1-\beta_2)\sum_{t = -\infty}^T \beta_2^{T-t} 4c^2x_t^2$$
$$v_t = (1-\beta_2)\sum_{t = -\infty}^T \beta_2^{T-t} 4c^2(x^*)^2 = 4c^2(x^*)^2$$

Finally, since we know that $x_{T+1} = -x_T = x^*$, we can use the update rule itself to write:

$$x_{T+1} = x_T - \eta\frac{m_T}{\sqrt{v_T} + \epsilon}$$

$$2x^* = \eta \frac{1-\beta_1}{1+\beta_1}\frac{2cx^*}{\epsilon + 2cx^*}$$

<!--When $\epsilon$ is very small ($\epsilon \ll c\eta(1-\beta_1)/(1+\beta_1)$), --> 
This equation admits a beautiful solution: 

$$x^* = \frac{\eta(1-\beta_1)}{2(1+\beta_1)} - \frac{\epsilon}{2c}$$

Note that when $\epsilon$ is small ($\epsilon \ll c\eta(1-\beta_1)/(1+\beta_1)$), the fixed point is completely independent of $c$ (and therefore the loss function), depending only on Adam's hyperparameters. If $\epsilon$ gets larger, the nontrivial fixed point will start becoming smaller and smaller until it reaches 0 when $\epsilon = c\eta (1-\beta_1)/(1-\beta_2)$.

If Adam's fixed points only exist in the noiseless setting, and generally aren't reached by Adam even then, why bother studying them? The answer is that they give us a sense for how close Adam can possibly get to the optimum, given the chosen set of hyperparameters. This matters because it would be embarrassing for us to choose a set of hyperparameters that result in us generally staying further from the optimum than would be implied by the level of noise.

To be more specific, when we run Adam on the noisy 1D quadratic, our ability to get as close as possible to the optimum is constrained by two factors simultaneously:

- Signal-to-noise limitations: we won't be able to get within less than $\sigma_{\xi}/(2c)$ of the minimum.
- Adam limitations: we won't (stably) be able to get within less than $x^*$ of the minimum.

This document is essentially about the second category of limitations, but such limitations only become relevant in the first place when $\sigma_{\xi}/(2c) < x^*$---that is, in the low-noise regime. I should emphasize that in many practical models, noise is sufficiently high that this effect doesn't appear to come into play.

## Adam's Instability

The title of this memo promised not just nonconvergence, but instability. Indeed, if the problem was just that Adam doesn't converge on quadratic loss functions, that would be one thing, but in fact it's worse than that: when the noise level is sufficiently low relative to the signal along one of the parameters (as formulated in the previous section), Adam will generally become unstable, repeatedly becoming much closer to the optimum than its fixed point, and then diverging. This section is about how and why that happens.

Let's start by unrolling $v_T$. (Once again, we'll be studying the steady-state behavior, and assuming $v_T = \hat{v}_T$.) We can write:

$$v_T = (1-\beta_2)\sum_{t = 0}^T \beta_2^{T-t} g_t^2$$
$$v_T = (1-\beta_2)\beta_2^T\sum_{t = 0}^T \beta_2^{-t} g_t^2$$

Let's consider the case where Adam is converging with a convergence rate faster than $\sqrt{\beta_2^t}$. (With Adam's default values, this is the typical case---and in any case this premise will soon justify itself.) Given this assumption, we can infer that the sequence $g_t^2$ is shrinking faster than the sequence $\beta_2^{-t}$ is growing, and that the overall sum of $\beta_2^{-t} g_t^2$ is dominated by its very earliest terms.

If this is the case, then the value of the sum isn't growing with $T$ for a reasonably large $T$, but rather is constant. Let's let $\sum_{t = 0}^T \beta_2^{-t} g_t^2 \approx \sum_{t = 0}^{\infty} \beta_2^{-t} g_t^2 = S$. Now we have $v_T = S(1-\beta_2)\beta_2^T$. Recall Adam's update rule:

$$x_{T+1} = x_T - \eta\frac{m_T}{\sqrt{v_T} + \epsilon}$$

Let's define the "effective learning rate" $\eta'(T)$ to be:

$$\eta'(T) = \frac{\eta}{\sqrt{v_T} + \epsilon}$$

We can rewrite Adam's update rule as follows:

$$x_{T+1} = x_T - \eta'(T)\sum_{t=0}^T\beta_1^{T-t}g_t$$

Let's also assume that $\epsilon \ll \sqrt{v_T}$ for now, and ignore it. (Don't worry, we'll revisit $\epsilon$ later.) 

$$\eta'(T) \approx \frac{\eta}{\sqrt{S(1-\beta_2)\beta_2^T}}$$

This no longer looks like Adam---this is SGD with momentum! The only difference is that instead of having a fixed learning rate, our learning rate varies with $T$. In fact, our learning rate *increases exponentially* with $T$: every time step, our learning rate is multiplied by a factor of $1/\sqrt{\beta_2}$. So effectively, when we run Adam and it converges faster than $\sqrt{\beta_2^t}$, we're more or less just running SGD with momentum---but with an exponentially increasing learning rate. Small wonder we experience instability! No matter how low we set $\eta$, eventually $\eta'(T)$ will increase to the point that SGD with momentum *will* diverge.

Let's revisit the assumptions I made now. First, will Adam actually converge faster than $\sqrt{\beta_2^T}$? Assuming it does, we're effectively running SGD with momentum with a varying learning rate, but thankfully that is surprisingly easy to analyze. With a fairly large $\beta_1$ like $\beta_1 = 0.9$, our convergence rate will be (mostly) perfectly constant while we are converging, with a convergence rate of $\sqrt{\beta_1^T}$. (For more details on this, I highly encourage reading Goh's [*Why Momentum Really Works*](https://distill.pub/2017/momentum/); in this section I rely on the beginning of the section titled "The Dynamics of Momentum," the accompanying demo, and footnote 4 (which is where the convergence rate of $\sqrt{\beta_1}$ is given)).

In any case, the take-away is that yes, the assumption justifies itself; given default Adam values of $\beta_1 = 0.9$ and $\beta_2 = 0.999$ (or, indeed, any $\beta_1 < \beta_2$), if we assume that Adam converges faster than $\sqrt{\beta_2^T}$, we can conclude that Adam will converge faster than $\sqrt{\beta_2^T}$.

Up to a point, that is. Once it's the case that $c\eta'(T) > 1 + \beta_1$ (and it has to happen sooner or later), our SGD with momentum will diverge. (Once again, thanks to [Goh's paper](https://distill.pub/2017/momentum/) for the relevant convergence analysis.) During this stage, the divergence rate is unfortunately much more complicated to analyze, as it depends on the varying $\eta'(T)$. By combining the formula at the bottom of Goh's footnote 4 with our formula for $\eta'(T)$, it is possible to work out the divergence rate as a function of time, and the approximate amount of time the divergence stage must last; this memo will exclude that analysis, though, because it is both very complicated and not terribly insightful. Suffice it to say that after having reached the point where $c\eta'(T) > 2 + 2\beta_1$, Adam won't diverge forever; rather, it will diverge until the point when the sum $\sum_{t = 0}^T \beta_2^{T-t} g_t^2$ stops being dominated by its earliest terms, and starts being dominated by its latest ones. At that point, Adam will start to converge again at a rate of $\sqrt{\beta_1^T}$, and the cycle will begin anew.

There's one more thing that I promised to address that I've ignored until now: $\epsilon$. Nonzero $\epsilon$ guarantees a maximum possible effective learning rate; no matter how small $v_T$ becomes, the effective learning rate can never be greater than $\eta (1-\beta_1)/\epsilon$. This means that that the unstable divergent behavior will only actually be observed if $c\eta (1-\beta_1)/\epsilon > 2 + 2\beta_1$. Note that for the default values of Adam ($\eta = 3\times 10^{-4}$, $\beta_1=0.9$, $\epsilon = 10^{-8}$), we will still observe periodic divergence for all but the least steep quadratics (divergence will occur when $c > 0.00127$). However, we do see that given nonzero $\epsilon$, we can fix Adam's instability by increasing $\beta_1$, decreasing $\eta$, or increasing $\epsilon$. In practice, if you think you are experiencing instability from this phenomenon (though not otherwise), I recommend increasing $\epsilon$, perhaps to somewhere between $10^{-3}$ and $10^{-5}$.

## Examples

As a worked example, let's look at the specific case of Adam run on the 1D noisy quadratic ($y = cx^2$, $g_t = 2cx_t + \xi$) with quadratic scaling $c = 1$ and noise standard deviation $\sigma_{\xi} = 10^{-8}$. We'll run Adam for $10^5$ time-steps with its default values of $\eta = 3\times 10^{-4}$, $\beta_1=0.9$, $\beta_2=0.999$, $\epsilon = 10^{-8}$, beginning with an initial value of $x_0 = 1$.
 
The first thing we should do is look at its nontrivial fixed point. In this case, we have $\epsilon \ll c\eta(1-\beta_1)/(1+\beta_1)$, so we can make use of the simpler approximate formula for the value of the fixed point: $x^* = \frac{\eta(1-\beta_1)}{2(1+\beta_1)}$. This gives us a fixed point of ß. We notice that this is much higher than $\sigma_\xi/(2c)$, and thereby infer that the limitations implied by our fixed point are much stronger than those implied by the signal-to-noise ratio. We can expect instability.

Below is a plot of $x_t$ for the first 25,000 time-steps. After a slightly slow start, Adam gets into the convergent regime and quickly converges at a rate of $\sqrt{\beta_1^t}$ to the limit implied by the signal-to-noise ratio, seemingly in contradiction to the prediction we made about the dynamics around the fixed point dominating. The only sign that anything is amiss is that the average value of $x$ seems to be creeping upward at a slow but exponential rate of $1/\sqrt{\beta_2^t}$. From our previous analysis, we know that this is because the effective learning rate is increasing exponentially, and although it hasn't yet caused divergence, it's making the noise have a more and more pronounced effect.

![x25k](figs/first_25k_x.png)

*Figure 1. On the left, we have $x$ as a function of time, and on the right we have  $\log_{10}(|x|)$, for the first 25,000 time-steps.*

Now let's see how the learning rates are doing. 

![LR25k](figs/first_25k_lrs.png)

*Figure 2. On the left, we have the effective learning rate $\eta'(t)$ as compared to the critical learning rate $(1+\beta_1)/c$, and on the right we have the same thing on a log scale, for the first 25,000 time-steps.*

Only by looking at this plot do we see just how close the impending disaster is. The effective learning rate $\eta'(t)$ is given by $\eta(1-\beta_1)/(\sqrt{v_t} + \epsilon)$. As we can see, after the beginning it increases exponentially at a rate of $1/\sqrt{\beta_2^t}$, as we predicted it would. When it exceeds the critical learning rate of $(1+\beta_1)/c$, divergence will occur. In trying to get closer to the minimum than Adam's fixed point allows, we were hubristic; that hubris must ultimately spell our downfall.

Let's see what happens over the course of the next 1,000 time-steps.

![x25k](figs/next_1k_x.png)
*Figure 3. On the left, we have $x$ as a function of time, and on the right we have  $\log_{10}(|x|)$, for time-steps between 25,000 and 26,000.*

![x25k](figs/next_1k_lrs.png)
*Figure 4. On the left, we have the effective learning rate $\eta'(t)$ as compared to the critical learning rate $(1+\beta_1)/c$, and on the right we have the same thing on a log scale, for time-steps between 25,000 and 26,000.*

Now we see the predicted periodic spiking behavior. Our spikes are composed of two phases: a converging phase during which we converge to the minimum at a rate of $\sqrt{\beta_1^t}$ while our effective learning rate increases at a rate of $1/\sqrt{\beta_2^t}$, until our effective learning rate surpasses the critical learning rate, at which point the diverging phase begins. During the diverging phase, we diverge at a hard-to-express, non-constant rate given by footnote 4 of Goh, until $x$ reaches an absolute value commensurate with previous peaks and begins converging again. With our choices of hyperparameters and noise, the peaks appear to have a period of about 190 time-steps, and continue at this rate for all $10^5$ time-steps that we run Adam for. It's possible that as this happens we make (extremely) slow progress towards Adam's nontrivial fixed point, but I don't see any evidence for it from looking at the plot, nor am I convinced that that is necessarily the outcome given the presence of noise.

How is the spikes' period affected by the hyperparameters? To work out the precise dependence is very difficult, because it requires understanding the complicated dynamics that exist at the boundaries between the convergent and divergent phases, as well as the complicated non-constant divergence rate; however, a few things seem to be true to me:

- Changing the learning rate $\eta$ makes no difference to the spikes' period, provided you don't set it so low relative to $\epsilon$ and $\beta_1$ that they disappear entirely.
- Increasing $\beta_1$ makes the spikes less frequent, because it makes convergence slower (convergence happens as $\sqrt{\beta_1^t})$.
- Increasing $\beta_2$ makes spikes less frequent, because it causes our effective learning rate to increase more slowly (our effective learning rate increases as $1/\sqrt{\beta_2^t}$).
- Changing $\epsilon$ makes no difference to the spikes' period, provided you don't set it so high relative to $\eta$ and $\beta_1$ that they disappear entirely.

<!-- What's the best solution to this problem? The simplest and easiest solution is to increase $\epsilon$. As we saw in our study of Adam's fixed points, when $\epsilon \ge c\eta (1-\beta_1)/(1-\beta_2)$, the model's fixed point reaches 0, and convergence becomes possible. This can be thought of as switching to SGD with momentum when the last observed gradients are sufficiently small, and that approach will work if you set your effectively learning rate low enough (equivalently, setting $\epsilon$ high enough). -->

## Adam's instability in "real" models

<!-- Does this issue come up in practice? Probably more often than never. Karl Cobbe, who works on my team, generously showed me a small toy model he used to look into dynamics around supervised learning; the model uses Adam for image classification and has 217,866 parameters.

Running his model yields the following loss curves:

![LR25k](figs/octokarl_both.png)
*Left: Karl's model with $\beta_2 = 0.99$ and other parameters set to default Adam parameters. Right: Karl's model with $\beta_2 = 0.999$ and other parameters set to default Adam parameters.*

These loss curves bear a strong resemblance to the following plots of Adam run on the 1D quadratic loss function: -->

Does Adam's instability matter in practice? In fact, yes. It's easier than you might think to find this phenomenon occurring in real neural nets. For instance, running the example MNIST training code found [here](https://www.tensorflow.org/tutorials) for more epochs yields the following loss curve:

![adam_standard_loss](figs/mnist_standard_loss.png)

We don't see a regular pattern of spikes, but that isn't enough to disqualify the spikes/noise we do see as being caused in part by Adam's instability. Recall that, assuming small $\epsilon$, we know that Adam's fixed point is $x^* = \pm \frac{\eta(1-\beta_1)}{2(1+\beta_1)}$. Let's try plotting the updates of a single parameter as a function of time, long after convergence (to isolate the effect we're looking for, this is from full-batch training, although the other plots are for standard batch-size-32 training):

![mnist_spikes](figs/mnist_spikes_5.png)

We see spikes, but not regular ones. Are these spikes a result of Adam's instability?

It depends on what you mean by "a result of." The spikes aren't regular, so there must be more at play than what's going on in the 1D quadratic model... but on the other hand, we can see that our weights are trying to converge to within less than $x^*$ of the true optimum (much of the time, we have $|u_t| < x^*$), and we know that that can't ever possibly happen when we use Adam. So while there is an object-level reason why we're seeing these spikes (in this case, it's a dropout layer), we also know that no matter what we did the universe would have found some excuse to keep us from converging all the way to the optimum.

If we delete the dropout layer, we revert to seeing the regular spiking pattern we've come to know and love:

![adam_spiky_loss](figs/mnist_spiky_loss.png)

## How to fix the problem

What's the best way to fix this issue? It depends on exactly what exactly you're finding to be problematic. If you don't care about converging to within less than $x^*$ of the loss function's optimum, but you just want to avoid the instability, then decreasing $\beta_2$ will have the effect of making your instability spikes smaller (and more frequent), and when you make it equal to $\beta_1$ they'll disappear altogether (since then our old assumption that Adam will converge faster than $\sqrt{\beta_2^t}$ will become false). This will have the added benefit of suppressing spikes from large observed gradients (see [this memo](https://go/memos/doc/spike_mitigation) for more details on that).

If you do want to converge arbitrarily close to the optimum, I suggest two possible easy fixes: the first is to use a learning rate schedule, and the second is to increase $\epsilon$.

Using a learning rate schedule means decaying the learning rate $\eta$ over the course of training. Given that Adam's instability comes from the fact that when you converge quickly, the effective learning rate increases by a factor of $1/\sqrt{\beta_2}$ each update, it seems natural to multiply $\eta$ by $\sqrt{\beta_2}$ each update. This will allow perfect fast convergence on the 1D quadratic model, but the absence of noise in the 1D quadratic model means this approach won't generalize directly to real-world models; most likely the correct approach will be to multiply $\eta$ by $\sqrt{\beta_2}$ every time you observe some number of batches, assuming your batches are already critically-sized (meaning they're at the size where your signal-to-noise ratio is 1); perhaps that number should be 10 or 100, although I haven't tested it extensively on many different problems. Unfortunately, it will probably depend to a certain degree on your problem and how large of a distance your model expects to travel in parameter-space from initialization to optimum; this, in turn, probably depends on how overparametrized your model is (with more overparametrized models having a shorter distance from initialization to optimum and therefore being able to tolerate a more aggressive learning rate decay). In any case, I'll note that it's probably better to err on the side of decaying the learning rate more slowly, because the consequence of decaying it too slowly is that you'll converge to the optimum more slowly than you could have, but the consequence of decaying it too quickly is that you'll converge to a suboptimal point.

Increasing $\epsilon$ is the other possible solution. I don't favor it as much, because it has a handful of downsides. The first is that you have to tune it to your problem (admittedly, you'll also probably have to tune it to your learning rate schedule). Increasing $\epsilon$ is like switching from Adam to SGD with momentum once your average gradients drop below a certain level; while this can be justified, different functions will have different levels of steepness and want different switching points. Vanilla SGD and SGD with momentum, after all, can be quite unstable and are very sensitive to the choice of learning rate (see the corresponding plots in [this memo](https://go/memos/doc/spike_mitigation)).

The second downside is that epsilon doesn't respect the different scales of your individual parameters. It might turn out that there's no single good value of $\epsilon$; maybe varying different parameters changes your loss function by different amounts, and you'd need a different $\epsilon$ for each parameter. This, after all, is the problem Adam was supposed to solve in the first place. By increasing $\epsilon$, you're partially unsolving it.

The final downside is that increasing $\epsilon$ by enough to get true convergence on all parameters will probably harm your performance. By increasing $\epsilon$, you're decreasing your effective learning rate; to mitigate this, you could try increasing $\eta$ in tandem with increasing $\epsilon$. I think that's the right thing to do, but it does mean there's another parameter for you to tune. 

Some slow fixes might be to try switching to a different search algorithm altogether; AdaGrad doesn't suffer from this issue (since it's effectively choosing $\beta_2$ to be 1, it kind of already has a built-in learning rate schedule of $\sqrt{\beta_2^t}$). And if KFAC suffers from this issue, it's too complicated for me to realize (and I doubt it does; in general, second-order methods are stable on quadratics, even noisy ones).

Here's a couple plots showing loss curves from applying the learning-rate schedule and higher-$\epsilon$ solutions to the MNIST example from the previous section. The learning-rate schedule displays much better performance in this example, but I don't strongly endorse this as representative; I didn't tune either very carefully (I used $\epsilon = 10^{-2}$ and a decay rate of $\sqrt{\beta_2}$ every 25 batches). Both solutions cause the periodic spikes to disappear.

![adam_decaying_lr](figs/mnist_decaying_lr.png)

*Figure 7. The MNIST tutorial, this time with a decaying learning rate to make Adam stable.*

![adam_large_eps](figs/mnist_large_eps.png)

*Figure 8. The MNIST tutorial, this time with a large $\epsilon$ to make Adam stable.*

## Acknowledgements

Thanks a lot to Chris Hesse, Jacob Hilton, and John Schulman, for tons and tons of help and ideas. Thanks also to Paul Christiano, Karl Cobbe, Gabriel Goh, Nick Ryder, and Peter Zhokov.

## TL;DR<a name="tldr"></a>

Adam is unstable. Is this a real problem for the model you're currently using? Here's how you find out:

- If your loss curves have periodic spikes after convergence, it's probably this issue---especially if their period varies as you vary $\beta_1$ and $\beta_2$ (for example, if making $\beta_2$ smaller makes the spikes smaller and more frequent).

- Your loss spikes are coming from divergence over many time-steps, rather than a single big observed gradient, that makes it likelier to be this issue.

- If you want to know if you're bottlenecked by this issue, try plotting update size against $\eta(1-\beta_1)/(1-\beta_2)$. If many of your updates are smaller than $\eta(1-\beta_1)/(1-\beta_2)$, and you chose $\epsilon$ to be small, then this issue is very likely bottlenecking you.

<!-- Is Adam's instability causing you problems? The biggest tells (in order from most obvious to least obvious) are:

- Periodic spikes; the slam dunk is if their period varies when you vary $\beta_1$ and $\beta_2$ (increasing either should cause the spikes to become less frequent, decreasing either should cause them to become more frequent, unless you set $\beta_1$ to 0 in which case complicated stuff happens).
- Your spikes are coming from divergent behavior over many time-steps, rather than a single large observed gradient.
- You don't have much noise in your model (a very large batch size, or not using batches at all?). -->

What can you do about it? Here are some possible quick fixes that shouldn't impact your performance too much:


- Try $\beta_1 = \beta_2 = 0.9$, or $\beta_1 = \beta_2 = 0.99$.
- Try using an exponentially decaying learning rate; if you're at the critical batch size, try decaying the learning rate by a factor of $\sqrt{\beta_2}$ every 10 or 100 batches. 
- Increase $\epsilon$ (try $\epsilon$ in the $10^{-2}$ to $10^{-4}$ range; if performance suffers, try increasing your learning rate a bit)
- Try adding Gaussian noise to your gradients.

Here are some more quick fixes that will negatively impact your performance, in case the previous ones didn't work.

- Try turning up $\beta_1$ a lot (say $\beta_1 = \beta_2 = 0.999$)
- Try turning down the learning rate $\eta$ a lot.

Here are some slow fixes that will hopefully do very well:

- Try setting $v_t$ to the be the max of two $v_t$'s controlled by two different $\beta_2$ parameters (one equal to $0.999$, the other equal to $0.9$)
- Try using AdaGrad instead of Adam.
  </p>
</d-article>



<d-appendix>

  <h3>Acknowledgments</h3>
  <p>
        We are grateful to Ludwig Schubert, Justin Gilmer, Shan Carter, and John Schulman for feedback on early drafts of this article. Shan also gave invaluable design advice on diagrams and interactive components. Thanks to Cassandra Xia for helping develop the playground demo.
  </p>

  <p>
    Sam would like to thank the <a href="https://ai.google/research/join-us/ai-residency/">Google AI Residency Program</a> for offering continuous support and guidance as he explored this line of research.
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Concepts:</b> Chris introduced the central concept and structure of this article. Working together, Sam and Chris fleshed out the finer points.
  </p>
  <p>
    <b>Writing & Diagrams:</b> The text and figures were initially drafted by Sam and then refined under Chris's guidance. The code used for the hero and playground visualizations was written by Chris and Cassandra and modified by Sam to fit the Distill format.
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>

